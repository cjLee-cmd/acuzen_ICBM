aiCommand
  .command('status')
  .description('Check AI model status and performance')
  .action(async () => {
    try {
      console.log('ğŸ¤– AI Model Status Report');
      console.log('========================\n');
      
      const models = await aiModelManager.getAllModels();
      
      for (const model of models) {
        console.log(`ğŸ“‹ Model: ${model.name}`);
        console.log(`   Version: ${model.version}`);
        console.log(`   Status: ${model.status}`);
        console.log(`   Accuracy: ${(model.accuracy * 100).toFixed(2)}%`);
        console.log(`   Last Updated: ${model.lastUpdated}`);
        console.log(`   Performance: ${model.avgResponseTime}ms avg response time`);
        console.log('');
      }
      
    } catch (error) {
      console.error('âŒ Failed to get AI model status:', error);
      process.exit(1);
    }
  });

aiCommand
  .command('retrain')
  .option('-m, --model <name>', 'Model name to retrain')
  .option('-d, --data <path>', 'Training data path')
  .description('Retrain AI model with new data')
  .action(async (options) => {
    try {
      if (!options.model) {
        console.error('âŒ Model name is required');
        process.exit(1);
      }
      
      console.log(`ğŸ”„ Starting retraining for model: ${options.model}`);
      
      const retrainingJob = await aiModelManager.retrainModel(options.model, {
        dataPath: options.data,
        validationSplit: 0.2,
        batchSize: 32,
        epochs: 10
      });
      
      console.log(`âœ… Retraining job started: ${retrainingJob.id}`);
      console.log(`ğŸ“Š Progress can be monitored at: /admin/ai/jobs/${retrainingJob.id}`);
      
    } catch (error) {
      console.error('âŒ Model retraining failed:', error);
      process.exit(1);
    }
  });

aiCommand
  .command('deploy')
  .option('-m, --model <name>', 'Model name to deploy')
  .option('-v, --version <version>', 'Model version to deploy')
  .option('--canary <percentage>', 'Canary deployment percentage', '10')
  .description('Deploy AI model to production')
  .action(async (options) => {
    try {
      if (!options.model) {
        console.error('âŒ Model name is required');
        process.exit(1);
      }
      
      console.log(`ğŸš€ Deploying model: ${options.model}@${options.version || 'latest'}`);
      
      const deployment = await aiModelManager.deployModel({
        modelName: options.model,
        version: options.version,
        strategy: 'canary',
        canaryPercentage: parseInt(options.canary)
      });
      
      console.log(`âœ… Model deployed successfully`);
      console.log(`ğŸ¯ Canary deployment: ${options.canary}% traffic`);
      console.log(`ğŸ“ˆ Monitor deployment at: /admin/ai/deployments/${deployment.id}`);
      
    } catch (error) {
      console.error('âŒ Model deployment failed:', error);
      process.exit(1);
    }
  });

// ì‚¬ìš©ì ê´€ë¦¬ ëª…ë ¹ì–´
const userCommand = program.command('user').description('User management commands');

userCommand
  .command('create')
  .option('-e, --email <email>', 'User email')
  .option('-r, --role <role>', 'User role', 'USER')
  .option('-o, --org <organization>', 'Organization')
  .description('Create new user')
  .action(async (options) => {
    try {
      if (!options.email) {
        console.error('âŒ Email is required');
        process.exit(1);
      }
      
      // ì„ì‹œ íŒ¨ìŠ¤ì›Œë“œ ìƒì„±
      const tempPassword = encryptionService.generateRandomString(12);
      const hashedPassword = await encryptionService.hashPassword(tempPassword);
      
      const user = await prisma.user.create({
        data: {
          email: options.email,
          role: options.role.toUpperCase(),
          organization: options.org,
          password: hashedPassword,
          isActive: true,
          createdAt: new Date()
        }
      });
      
      console.log('âœ… User created successfully');
      console.log(`ğŸ“§ Email: ${user.email}`);
      console.log(`ğŸ”‘ Temporary password: ${tempPassword}`);
      console.log(`ğŸ‘¤ Role: ${user.role}`);
      console.log('âš ï¸  Please ask user to change password on first login');
      
    } catch (error) {
      console.error('âŒ User creation failed:', error);
      process.exit(1);
    }
  });

userCommand
  .command('list')
  .option('-r, --role <role>', 'Filter by role')
  .option('-a, --active', 'Show only active users')
  .description('List users')
  .action(async (options) => {
    try {
      const whereClause: any = {};
      
      if (options.role) {
        whereClause.role = options.role.toUpperCase();
      }
      
      if (options.active) {
        whereClause.isActive = true;
      }
      
      const users = await prisma.user.findMany({
        where: whereClause,
        select: {
          id: true,
          email: true,
          role: true,
          organization: true,
          isActive: true,
          lastLoginAt: true,
          createdAt: true
        },
        orderBy: {
          createdAt: 'desc'
        }
      });
      
      console.log(`ğŸ‘¥ Found ${users.length} users:\n`);
      
      users.forEach(user => {
        console.log(`ğŸ“§ ${user.email}`);
        console.log(`   Role: ${user.role}`);
        console.log(`   Organization: ${user.organization || 'N/A'}`);
        console.log(`   Status: ${user.isActive ? 'âœ… Active' : 'âŒ Inactive'}`);
        console.log(`   Last Login: ${user.lastLoginAt || 'Never'}`);
        console.log(`   Created: ${user.createdAt.toLocaleDateString()}`);
        console.log('');
      });
      
    } catch (error) {
      console.error('âŒ Failed to list users:', error);
      process.exit(1);
    }
  });

// ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§ ëª…ë ¹ì–´
const monitorCommand = program.command('monitor').description('System monitoring commands');

monitorCommand
  .command('health')
  .description('Check system health')
  .action(async () => {
    try {
      console.log('ğŸ¥ System Health Check');
      console.log('===================\n');
      
      // ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í™•ì¸
      try {
        await prisma.$queryRaw`SELECT 1`;
        console.log('âœ… Database: Connected');
      } catch (dbError) {
        console.log('âŒ Database: Connection failed');
        console.log(`   Error: ${dbError}`);
      }
      
      // Redis ì—°ê²° í™•ì¸
      try {
        const Redis = require('redis');
        const redis = Redis.createClient({ url: process.env.REDIS_URL });
        await redis.ping();
        await redis.quit();
        console.log('âœ… Redis: Connected');
      } catch (redisError) {
        console.log('âŒ Redis: Connection failed');
        console.log(`   Error: ${redisError}`);
      }
      
      // AI ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸
      try {
        const aiStatus = await aiModelManager.getServiceHealth();
        console.log(`âœ… AI Service: ${aiStatus.status}`);
        console.log(`   Models loaded: ${aiStatus.modelsLoaded}`);
        console.log(`   GPU available: ${aiStatus.gpuAvailable ? 'Yes' : 'No'}`);
      } catch (aiError) {
        console.log('âŒ AI Service: Health check failed');
        console.log(`   Error: ${aiError}`);
      }
      
      // ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ í™•ì¸
      const os = require('os');
      const totalMem = os.totalmem() / 1024 / 1024 / 1024; // GB
      const freeMem = os.freemem() / 1024 / 1024 / 1024; // GB
      const memUsage = ((totalMem - freeMem) / totalMem * 100).toFixed(1);
      
      console.log(`ğŸ’» System Resources:`);
      console.log(`   CPU Load: ${os.loadavg()[0].toFixed(2)}`);
      console.log(`   Memory Usage: ${memUsage}% (${(totalMem - freeMem).toFixed(1)}GB / ${totalMem.toFixed(1)}GB)`);
      console.log(`   Uptime: ${(os.uptime() / 3600).toFixed(1)} hours`);
      
    } catch (error) {
      console.error('âŒ Health check failed:', error);
      process.exit(1);
    }
  });

monitorCommand
  .command('performance')
  .option('-d, --days <days>', 'Number of days to analyze', '7')
  .description('Generate performance report')
  .action(async (options) => {
    try {
      const days = parseInt(options.days);
      const startDate = new Date();
      startDate.setDate(startDate.getDate() - days);
      
      console.log(`ğŸ“Š Performance Report (Last ${days} days)`);
      console.log('=====================================\n');
      
      // ì¼€ì´ìŠ¤ ì²˜ë¦¬ í†µê³„
      const caseStats = await prisma.case.groupBy({
        by: ['status'],
        _count: { status: true },
        where: {
          createdAt: {
            gte: startDate
          }
        }
      });
      
      console.log('ğŸ“‹ Case Processing Statistics:');
      caseStats.forEach(stat => {
        console.log(`   ${stat.status}: ${stat._count.status} cases`);
      });
      
      // AI ì²˜ë¦¬ ì„±ëŠ¥
      const aiStats = await prisma.aIPrediction.aggregate({
        _avg: {
          confidence: true,
          processingTime: true
        },
        _count: {
          id: true
        },
        where: {
          createdAt: {
            gte: startDate
          }
        }
      });
      
      console.log('\nğŸ¤– AI Processing Performance:');
      console.log(`   Total predictions: ${aiStats._count.id}`);
      console.log(`   Average confidence: ${(aiStats._avg.confidence || 0 * 100).toFixed(2)}%`);
      console.log(`   Average processing time: ${(aiStats._avg.processingTime || 0).toFixed(2)}s`);
      
      // ì—ëŸ¬ìœ¨ ê³„ì‚°
      const totalRequests = await prisma.auditLog.count({
        where: {
          timestamp: {
            gte: startDate
          }
        }
      });
      
      const errorRequests = await prisma.auditLog.count({
        where: {
          timestamp: {
            gte: startDate
          },
          action: {
            contains: 'ERROR'
          }
        }
      });
      
      const errorRate = totalRequests > 0 ? (errorRequests / totalRequests * 100).toFixed(2) : '0.00';
      
      console.log('\nğŸ“ˆ System Reliability:');
      console.log(`   Total requests: ${totalRequests}`);
      console.log(`   Error requests: ${errorRequests}`);
      console.log(`   Error rate: ${errorRate}%`);
      
    } catch (error) {
      console.error('âŒ Performance report generation failed:', error);
      process.exit(1);
    }
  });

// ìœ ì§€ë³´ìˆ˜ ëª…ë ¹ì–´
const maintenanceCommand = program.command('maintenance').description('System maintenance commands');

maintenanceCommand
  .command('cleanup')
  .option('--dry-run', 'Show what would be deleted without actually deleting')
  .option('--days <days>', 'Delete logs older than specified days', '90')
  .description('Clean up old logs and temporary files')
  .action(async (options) => {
    try {
      const days = parseInt(options.days);
      const cutoffDate = new Date();
      cutoffDate.setDate(cutoffDate.getDate() - days);
      
      console.log(`ğŸ§¹ Cleanup Operation (${options.dryRun ? 'DRY RUN' : 'LIVE'})`);
      console.log(`ğŸ“… Removing data older than ${days} days (before ${cutoffDate.toISOString()})`);
      console.log('='.repeat(60));
      
      // ì˜¤ë˜ëœ ê°ì‚¬ ë¡œê·¸ ì •ë¦¬
      const oldAuditLogs = await prisma.auditLog.findMany({
        where: {
          timestamp: {
            lt: cutoffDate
          }
        },
        select: { id: true }
      });
      
      console.log(`ğŸ“‹ Found ${oldAuditLogs.length} old audit logs`);
      
      if (!options.dryRun && oldAuditLogs.length > 0) {
        await prisma.auditLog.deleteMany({
          where: {
            timestamp: {
              lt: cutoffDate
            }
          }
        });
        console.log(`âœ… Deleted ${oldAuditLogs.length} audit logs`);
      }
      
      // ì˜¤ë˜ëœ AI ì˜ˆì¸¡ ì •ë¦¬ (ë‚®ì€ ì‹ ë¢°ë„)
      const lowConfidencePredictions = await prisma.aIPrediction.findMany({
        where: {
          createdAt: {
            lt: cutoffDate
          },
          confidence: {
            lt: 0.5
          },
          humanReviewed: false
        },
        select: { id: true }
      });
      
      console.log(`ğŸ¤– Found ${lowConfidencePredictions.length} low-confidence AI predictions`);
      
      if (!options.dryRun && lowConfidencePredictions.length > 0) {
        await prisma.aIPrediction.deleteMany({
          where: {
            createdAt: {
              lt: cutoffDate
            },
            confidence: {
              lt: 0.5
            },
            humanReviewed: false
          }
        });
        console.log(`âœ… Deleted ${lowConfidencePredictions.length} low-confidence predictions`);
      }
      
      // ì„ì‹œ íŒŒì¼ ì •ë¦¬
      const fs = require('fs').promises;
      const path = require('path');
      const tempDir = './temp';
      
      try {
        const files = await fs.readdir(tempDir);
        let deletedFiles = 0;
        
        for (const file of files) {
          const filePath = path.join(tempDir, file);
          const stats = await fs.stat(filePath);
          
          if (stats.mtime < cutoffDate) {
            if (!options.dryRun) {
              await fs.unlink(filePath);
            }
            deletedFiles++;
          }
        }
        
        console.log(`ğŸ—‚ï¸  Found ${deletedFiles} old temporary files`);
        if (!options.dryRun) {
          console.log(`âœ… Deleted ${deletedFiles} temporary files`);
        }
        
      } catch (error) {
        console.log('âš ï¸  No temp directory or temp files found');
      }
      
      if (options.dryRun) {
        console.log('\nğŸ’¡ This was a dry run. Use without --dry-run to actually perform cleanup.');
      } else {
        console.log('\nâœ… Cleanup completed successfully');
      }
      
    } catch (error) {
      console.error('âŒ Cleanup failed:', error);
      process.exit(1);
    }
  });

maintenanceCommand
  .command('vacuum')
  .description('Optimize database storage')
  .action(async () => {
    try {
      console.log('ğŸ”„ Running database vacuum...');
      
      await prisma.$executeRawUnsafe('VACUUM ANALYZE;');
      
      console.log('âœ… Database vacuum completed');
      console.log('ğŸ’¡ Database storage optimized and statistics updated');
      
    } catch (error) {
      console.error('âŒ Database vacuum failed:', error);
      process.exit(1);
    }
  });

// ë©”ì¸ ì‹¤í–‰
program.parse();

// í”„ë¡œê·¸ë¨ ì¢…ë£Œ ì‹œ ì •ë¦¬
process.on('SIGINT', async () => {
  console.log('\nğŸ›‘ Shutting down gracefully...');
  await prisma.$disconnect();
  process.exit(0);
});

process.on('SIGTERM', async () => {
  console.log('\nğŸ›‘ Received SIGTERM, shutting down...');
  await prisma.$disconnect();
  process.exit(0);
});
```

### 10.2 ë°±ì—… ë° ì¬í•´ë³µêµ¬

#### **<ì½”ë“œ 23>** ìë™ ë°±ì—… ë° ë³µêµ¬ ìŠ¤í¬ë¦½íŠ¸
```bash
#!/bin/bash
# scripts/backup-restore.sh

set -e  # Exit on any error

# Configuration
BACKUP_DIR="/var/backups/icsr-ai"
S3_BUCKET="icsr-ai-backups"
RETENTION_DAYS=30
LOG_FILE="/var/log/icsr-ai-backup.log"

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
NC='\033[0m' # No Color

# Logging function
log() {
    echo "[$(date '+%Y-%m-%d %H:%M:%S')] $1" | tee -a "$LOG_FILE"
}

error() {
    echo -e "${RED}[ERROR] $1${NC}" | tee -a "$LOG_FILE"
}

success() {
    echo -e "${GREEN}[SUCCESS] $1${NC}" | tee -a "$LOG_FILE"
}

warning() {
    echo -e "${YELLOW}[WARNING] $1${NC}" | tee -a "$LOG_FILE"
}

# Create backup directory if it doesn't exist
mkdir -p "$BACKUP_DIR"

# Database backup function
backup_database() {
    log "Starting database backup..."
    
    local timestamp=$(date +"%Y%m%d_%H%M%S")
    local backup_file="$BACKUP_DIR/db_backup_$timestamp.sql"
    local compressed_file="$backup_file.gz"
    
    # Create database dump
    if pg_dump "$DATABASE_URL" > "$backup_file"; then
        success "Database dump created: $backup_file"
    else
        error "Failed to create database dump"
        return 1
    fi
    
    # Compress backup
    if gzip "$backup_file"; then
        success "Backup compressed: $compressed_file"
    else
        error "Failed to compress backup"
        return 1
    fi
    
    # Encrypt backup
    local encrypted_file="$compressed_file.enc"
    if openssl enc -aes-256-cbc -salt -in "$compressed_file" -out "$encrypted_file" -pass pass:"$BACKUP_ENCRYPTION_KEY"; then
        success "Backup encrypted: $encrypted_file"
        rm "$compressed_file"  # Remove unencrypted version
    else
        error "Failed to encrypt backup"
        return 1
    fi
    
    # Upload to S3
    if aws s3 cp "$encrypted_file" "s3://$S3_BUCKET/database/"; then
        success "Backup uploaded to S3"
    else
        error "Failed to upload backup to S3"
        return 1
    fi
    
    echo "$encrypted_file"
}

# File storage backup function
backup_files() {
    log "Starting file storage backup..."
    
    local timestamp=$(date +"%Y%m%d_%H%M%S")
    local backup_file="$BACKUP_DIR/files_backup_$timestamp.tar.gz"
    
    # Create tar archive of important files
    if tar -czf "$backup_file" \
        -C / \
        var/uploads \
        var/models \
        etc/ssl/certs \
        --exclude='*.tmp' \
        --exclude='*.log'; then
        success "File backup created: $backup_file"
    else
        error "Failed to create file backup"
        return 1
    fi
    
    # Encrypt file backup
    local encrypted_file="$backup_file.enc"
    if openssl enc -aes-256-cbc -salt -in "$backup_file" -out "$encrypted_file" -pass pass:"$BACKUP_ENCRYPTION_KEY"; then
        success "File backup encrypted: $encrypted_file"
        rm "$backup_file"  # Remove unencrypted version
    else
        error "Failed to encrypt file backup"
        return 1
    fi
    
    # Upload to S3
    if aws s3 cp "$encrypted_file" "s3://$S3_BUCKET/files/"; then
        success "File backup uploaded to S3"
    else
        error "Failed to upload file backup to S3"
        return 1
    fi
    
    echo "$encrypted_file"
}

# Configuration backup function
backup_config() {
    log "Starting configuration backup..."
    
    local timestamp=$(date +"%Y%m%d_%H%M%S")
    local backup_file="$BACKUP_DIR/config_backup_$timestamp.tar.gz"
    
    # Backup Kubernetes configs, environment files, etc.
    if tar -czf "$backup_file" \
        -C / \
        etc/kubernetes \
        opt/icsr-ai/config \
        home/admin/.env.production; then
        success "Configuration backup created: $backup_file"
    else
        error "Failed to create configuration backup"
        return 1
    fi
    
    # Upload to S3
    if aws s3 cp "$backup_file" "s3://$S3_BUCKET/config/"; then
        success "Configuration backup uploaded to S3"
    else
        error "Failed to upload configuration backup to S3"
        return 1
    fi
    
    echo "$backup_file"
}

# Cleanup old backups
cleanup_old_backups() {
    log "Cleaning up old backups..."
    
    # Local cleanup
    find "$BACKUP_DIR" -name "*.enc" -mtime +$RETENTION_DAYS -delete
    find "$BACKUP_DIR" -name "*.tar.gz" -mtime +$RETENTION_DAYS -delete
    find "$BACKUP_DIR" -name "*.sql.gz" -mtime +$RETENTION_DAYS -delete
    
    success "Local old backups cleaned up"
    
    # S3 cleanup (lifecycle policy should handle this, but double-check)
    local cutoff_date=$(date -d "$RETENTION_DAYS days ago" +%Y-%m-%d)
    aws s3api list-objects-v2 --bucket "$S3_BUCKET" --query "Contents[?LastModified<'$cutoff_date'].Key" --output text | \
    while read -r key; do
        if [ -n "$key" ]; then
            aws s3 rm "s3://$S3_BUCKET/$key"
            log "Deleted old S3 backup: $key"
        fi
    done
    
    success "S3 old backups cleaned up"
}

# Database restore function
restore_database() {
    local backup_file="$1"
    
    if [ -z "$backup_file" ]; then
        error "Backup file path is required for restore"
        return 1
    fi
    
    log "Starting database restore from: $backup_file"
    
    # Check if file exists
    if [ ! -f "$backup_file" ]; then
        error "Backup file not found: $backup_file"
        return 1
    fi
    
    # Decrypt backup
    local decrypted_file="${backup_file%.enc}"
    if openssl enc -aes-256-cbc -d -in "$backup_file" -out "$decrypted_file" -pass pass:"$BACKUP_ENCRYPTION_KEY"; then
        success "Backup decrypted successfully"
    else
        error "Failed to decrypt backup"
        return 1
    fi
    
    # Decompress backup
    local sql_file="${decrypted_file%.gz}"
    if gunzip "$decrypted_file"; then
        success "Backup decompressed successfully"
    else
        error "Failed to decompress backup"
        return 1
    fi
    
    # Create confirmation prompt
    warning "âš ï¸  This will COMPLETELY REPLACE the current database!"
    warning "Current database will be backed up before restore."
    read -p "Are you sure you want to continue? (yes/no): " confirm
    
    if [ "$confirm" != "yes" ]; then
        log "Restore cancelled by user"
        return 1
    fi
    
    # Backup current database before restore
    log "Creating safety backup of current database..."
    local safety_backup="$BACKUP_DIR/safety_backup_$(date +%Y%m%d_%H%M%S).sql"
    if pg_dump "$DATABASE_URL" > "$safety_backup"; then
        success "Safety backup created: $safety_backup"
    else
        error "Failed to create safety backup"
        return 1
    fi
    
    # Stop application services
    log "Stopping application services..."
    kubectl scale deployment/icsr-ai-backend --replicas=0 -n icsr-ai-prod
    kubectl scale deployment/icsr-ai-frontend --replicas=0 -n icsr-ai-prod
    
    # Wait for pods to stop
    sleep 30
    
    # Drop and recreate database
    log "Recreating database..."
    if psql "$DATABASE_URL" -c "DROP SCHEMA public CASCADE; CREATE SCHEMA public;"; then
        success "Database schema recreated"
    else
        error "Failed to recreate database schema"
        return 1
    fi
    
    # Restore from backup
    log "Restoring database from backup..."
    if psql "$DATABASE_URL" < "$sql_file"; then
        success "Database restored successfully"
    else
        error "Failed to restore database"
        
        # Attempt to restore safety backup
        warning "Attempting to restore from safety backup..."
        psql "$DATABASE_URL" < "$safety_backup"
        return 1
    fi
    
    # Restart application services
    log "Restarting application services..."
    kubectl scale deployment/icsr-ai-backend --replicas=3 -n icsr-ai-prod
    kubectl scale deployment/icsr-ai-frontend --replicas=2 -n icsr-ai-prod
    
    # Wait for services to be ready
    kubectl wait --for=condition=available --timeout=300s deployment/icsr-ai-backend -n icsr-ai-prod
    kubectl wait --for=condition=available --timeout=300s deployment/icsr-ai-frontend -n icsr-ai-prod
    
    # Cleanup temporary files
    rm -f "$sql_file"
    
    success "Database restore completed successfully"
}

# Health check after restore
health_check() {
    log "Performing post-restore health check..."
    
    local max_attempts=30
    local attempt=1
    
    while [ $attempt -le $max_attempts ]; do
        if curl -f -s "https://api.icsr-ai.company.com/health" > /dev/null; then
            success "Application health check passed"
            return 0
        fi
        
        log "Health check attempt $attempt/$max_attempts failed, retrying in 10s..."
        sleep 10
        ((attempt++))
    done
    
    error "Application failed health check after restore"
    return 1
}

# Send notification
send_notification() {
    local status="$1"
    local message="$2"
    
    # Slack notification
    if [ -n "$SLACK_WEBHOOK_URL" ]; then
        curl -X POST -H 'Content-type: application/json' \
            --data "{\"text\":\"ğŸ”„ ICSR-AI Backup Report\\n*Status:* $status\\n*Message:* $message\\n*Time:* $(date)\"}" \
            "$SLACK_WEBHOOK_URL"
    fi
    
    # Email notification (requires mailutils)
    if command -v mail >/dev/null 2>&1 && [ -n "$NOTIFICATION_EMAIL" ]; then
        echo "$message" | mail -s "ICSR-AI Backup Report - $status" "$NOTIFICATION_EMAIL"
    fi
}

# Main backup function
run_backup() {
    log "========== Starting ICSR-AI System Backup =========="
    
    local overall_status="SUCCESS"
    local error_messages=()
    
    # Database backup
    if ! db_backup_file=$(backup_database); then
        overall_status="PARTIAL_FAILURE"
        error_messages+=("Database backup failed")
    fi
    
    # File backup
    if ! file_backup_file=$(backup_files); then
        overall_status="PARTIAL_FAILURE"
        error_messages+=("File backup failed")
    fi
    
    # Configuration backup
    if ! config_backup_file=$(backup_config); then
        overall_status="PARTIAL_FAILURE"
        error_messages+=("Configuration backup failed")
    fi
    
    # Cleanup
    cleanup_old_backups
    
    # Generate backup report
    local backup_report="Backup Summary:\n"
    backup_report+="- Database: ${db_backup_file:-FAILED}\n"
    backup_report+="- Files: ${file_backup_file:-FAILED}\n"
    backup_report+="- Config: ${config_backup_file:-FAILED}\n"
    
    if [ ${#error_messages[@]} -gt 0 ]; then
        backup_report+="Errors:\n"
        printf '%s\n' "${error_messages[@]}" | sed 's/^/- /' >> /tmp/backup_report
        backup_report+="$(cat /tmp/backup_report)"
        rm -f /tmp/backup_report
    fi
    
    log "========== Backup Completed with status: $overall_status =========="
    
    # Send notification
    send_notification "$overall_status" "$backup_report"
    
    if [ "$overall_status" = "SUCCESS" ]; then
        return 0
    else
        return 1
    fi
}

# Command line interface
case "$1" in
    "backup")
        run_backup
        ;;
    "restore")
        if [ -z "$2" ]; then
            error "Usage: $0 restore <backup_file_path>"
            exit 1
        fi
        restore_database "$2"
        health_check
        ;;
    "cleanup")
        cleanup_old_backups
        ;;
    "health")
        health_check
        ;;
    *)
        echo "Usage: $0 {backup|restore <file>|cleanup|health}"
        echo ""
        echo "Commands:"
        echo "  backup  - Run full system backup"
        echo "  restore - Restore from backup file"
        echo "  cleanup - Clean up old backup files"
        echo "  health  - Check application| **ë°ì´í„° ì•”í˜¸í™”** | ì €ì¥/ì „ì†¡ ì¤‘ ì•”í˜¸í™” | AES-256, TLS 1.3 | FIPS 140-2 |
| **ì ‘ê·¼ ì œì–´** | ì—­í•  ê¸°ë°˜ ì ‘ê·¼ ì œì–´ | RBAC, ìµœì†Œ ê¶Œí•œ ì›ì¹™ | NIST 800-53 |
| **ê°œì¸ì •ë³´ ë³´í˜¸** | PII ìµëª…í™”/ê°€ëª…ì²˜ë¦¬ | í•´ì‹œ, í† í°í™” | GDPR, HIPAA |
| **ê°ì‚¬ ì¶”ì ** | ëª¨ë“  í™œë™ ë¡œê¹… | ë³€ì¡° ë°©ì§€ ë¡œê·¸ | SOX, CFR 21 Part 11 |
| **ë„¤íŠ¸ì›Œí¬ ë³´ì•ˆ** | ì¹¨ì… íƒì§€/ì°¨ë‹¨ | WAF, IDS/IPS | PCI DSS |
| **ì·¨ì•½ì  ê´€ë¦¬** | ì •ê¸° ë³´ì•ˆ ìŠ¤ìº” | SAST, DAST, SCA | OWASP Top 10 |

#### **<ì½”ë“œ 20>** ë³´ì•ˆ ë¯¸ë“¤ì›¨ì–´ êµ¬í˜„
```typescript
// src/middleware/securityMiddleware.ts
import { Request, Response, NextFunction } from 'express';
import rateLimit from 'express-rate-limit';
import helmet from 'helmet';
import { body, validationResult } from 'express-validator';
import crypto from 'crypto';
import jwt from 'jsonwebtoken';
import { AppError } from '../utils/AppError';
import { logger } from '../utils/logger';
import { RedisClient } from '../utils/redis';

export class SecurityMiddleware {
  private redis: RedisClient;

  constructor() {
    this.redis = new RedisClient();
  }

  // Helmet ë³´ì•ˆ í—¤ë” ì„¤ì •
  static securityHeaders() {
    return helmet({
      contentSecurityPolicy: {
        directives: {
          defaultSrc: ["'self'"],
          scriptSrc: ["'self'", "'unsafe-inline'", 'cdn.jsdelivr.net'],
          styleSrc: ["'self'", "'unsafe-inline'", 'fonts.googleapis.com'],
          fontSrc: ["'self'", 'fonts.gstatic.com'],
          imgSrc: ["'self'", 'data:', 'https:'],
          connectSrc: ["'self'"],
          frameSrc: ["'none'"],
          objectSrc: ["'none'"],
          upgradeInsecureRequests: []
        }
      },
      hsts: {
        maxAge: 31536000,
        includeSubDomains: true,
        preload: true
      },
      noSniff: true,
      frameguard: { action: 'deny' },
      xssFilter: true,
      referrerPolicy: { policy: 'strict-origin-when-cross-origin' }
    });
  }

  // API ì†ë„ ì œí•œ
  static createRateLimiter(windowMs: number = 15 * 60 * 1000, max: number = 100) {
    return rateLimit({
      windowMs,
      max,
      message: {
        error: 'Too many requests',
        message: 'Rate limit exceeded. Please try again later.',
        retryAfter: Math.ceil(windowMs / 1000)
      },
      standardHeaders: true,
      legacyHeaders: false,
      skip: (req) => {
        // í—¬ìŠ¤ ì²´í¬ ë“± íŠ¹ì • ì—”ë“œí¬ì¸íŠ¸ëŠ” ì œì™¸
        const skipPaths = ['/health', '/metrics'];
        return skipPaths.includes(req.path);
      },
      keyGenerator: (req) => {
        // IP ì£¼ì†Œì™€ ì‚¬ìš©ì ID ì¡°í•©ìœ¼ë¡œ í‚¤ ìƒì„±
        const userId = req.user?.id || 'anonymous';
        return `${req.ip}:${userId}`;
      }
    });
  }

  // JWT í† í° ê²€ì¦ ë¯¸ë“¤ì›¨ì–´
  static authenticateToken(req: Request, res: Response, next: NextFunction) {
    const authHeader = req.headers.authorization;
    const token = authHeader?.split(' ')[1]; // Bearer TOKEN

    if (!token) {
      throw new AppError('Access token required', 401);
    }

    jwt.verify(token, process.env.JWT_SECRET!, async (err, decoded) => {
      if (err) {
        logger.logSecurityEvent('Invalid JWT token', 'medium', {
          ip: req.ip,
          userAgent: req.headers['user-agent'],
          token: token.substring(0, 10) + '...' // ë¶€ë¶„ë§Œ ë¡œê¹…
        });
        throw new AppError('Invalid or expired token', 403);
      }

      const payload = decoded as any;
      
      // í† í° ë¸”ë™ë¦¬ìŠ¤íŠ¸ í™•ì¸
      const isBlacklisted = await SecurityMiddleware.isTokenBlacklisted(token);
      if (isBlacklisted) {
        throw new AppError('Token has been revoked', 403);
      }

      // ì‚¬ìš©ì ì •ë³´ ì„¤ì •
      req.user = {
        id: payload.sub,
        email: payload.email,
        role: payload.role,
        senderID: payload.senderID,
        permissions: payload.permissions || []
      };

      // í† í° í™œë™ ê¸°ë¡
      logger.info('Token authenticated', {
        userId: payload.sub,
        tokenId: payload.jti,
        ip: req.ip
      });

      next();
    });
  }

  // ì—­í•  ê¸°ë°˜ ì ‘ê·¼ ì œì–´
  static requireRole(...roles: string[]) {
    return (req: Request, res: Response, next: NextFunction) => {
      if (!req.user) {
        throw new AppError('Authentication required', 401);
      }

      if (!roles.includes(req.user.role)) {
        logger.logSecurityEvent('Unauthorized role access attempt', 'high', {
          userId: req.user.id,
          requiredRoles: roles,
          actualRole: req.user.role,
          path: req.path,
          ip: req.ip
        });
        throw new AppError('Insufficient permissions', 403);
      }

      next();
    };
  }

  // ê¶Œí•œ ê¸°ë°˜ ì ‘ê·¼ ì œì–´
  static requirePermission(...permissions: string[]) {
    return (req: Request, res: Response, next: NextFunction) => {
      if (!req.user?.permissions) {
        throw new AppError('Permissions not found', 403);
      }

      const hasPermission = permissions.every(permission => 
        req.user.permissions.includes(permission)
      );

      if (!hasPermission) {
        logger.logSecurityEvent('Unauthorized permission access attempt', 'high', {
          userId: req.user.id,
          requiredPermissions: permissions,
          actualPermissions: req.user.permissions,
          path: req.path,
          ip: req.ip
        });
        throw new AppError('Insufficient permissions', 403);
      }

      next();
    };
  }

  // ì…ë ¥ ê²€ì¦ ë° ì‚´ê· 
  static validateAndSanitize = {
    // ì´ë©”ì¼ ê²€ì¦
    email: body('email')
      .isEmail()
      .normalizeEmail()
      .withMessage('Valid email is required'),

    // íŒ¨ìŠ¤ì›Œë“œ ê²€ì¦
    password: body('password')
      .isLength({ min: 8, max: 128 })
      .matches(/^(?=.*[a-z])(?=.*[A-Z])(?=.*\d)(?=.*[@$!%*?&])[A-Za-z\d@$!%*?&]/)
      .withMessage('Password must be 8-128 characters with uppercase, lowercase, number, and special character'),

    // ì‚¬ìš©ì ì…ë ¥ ì‚´ê· 
    sanitizeString: body('*')
      .trim()
      .escape(),

    // SQL ì¸ì ì…˜ ë°©ì§€
    preventSQLInjection: body('*').custom((value) => {
      if (typeof value === 'string') {
        const sqlPatterns = [
          /(\b(SELECT|INSERT|UPDATE|DELETE|DROP|CREATE|ALTER|EXEC|EXECUTE)\b)/i,
          /(\b(UNION|OR|AND)\b.*\b(SELECT|INSERT|UPDATE|DELETE)\b)/i,
          /(--|\/\*|\*\/|;)/,
          /(\b(SCRIPT|JAVASCRIPT|VBSCRIPT)\b)/i
        ];

        for (const pattern of sqlPatterns) {
          if (pattern.test(value)) {
            throw new Error('Potentially malicious input detected');
          }
        }
      }
      return true;
    }),

    // XSS ë°©ì§€
    preventXSS: body('*').custom((value) => {
      if (typeof value === 'string') {
        const xssPatterns = [
          /<script\b[^<]*(?:(?!<\/script>)<[^<]*)*<\/script>/gi,
          /<iframe\b[^<]*(?:(?!<\/iframe>)<[^<]*)*<\/iframe>/gi,
          /javascript:/gi,
          /on\w+\s*=/gi
        ];

        for (const pattern of xssPatterns) {
          if (pattern.test(value)) {
            throw new Error('XSS attempt detected');
          }
        }
      }
      return true;
    })
  };

  // ê²€ì¦ ì˜¤ë¥˜ ì²˜ë¦¬
  static handleValidationErrors(req: Request, res: Response, next: NextFunction) {
    const errors = validationResult(req);
    
    if (!errors.isEmpty()) {
      logger.logSecurityEvent('Input validation failed', 'medium', {
        errors: errors.array(),
        ip: req.ip,
        path: req.path,
        userId: req.user?.id
      });

      throw new AppError('Input validation failed', 400, errors.array());
    }

    next();
  }

  // CSRF í† í° ìƒì„±
  static generateCSRFToken(): string {
    return crypto.randomBytes(32).toString('hex');
  }

  // CSRF í† í° ê²€ì¦
  static verifyCSRFToken(req: Request, res: Response, next: NextFunction) {
    const token = req.headers['x-csrf-token'] || req.body._csrf;
    const sessionToken = req.session?.csrfToken;

    if (!token || !sessionToken || token !== sessionToken) {
      logger.logSecurityEvent('CSRF token validation failed', 'high', {
        ip: req.ip,
        userAgent: req.headers['user-agent'],
        userId: req.user?.id
      });
      throw new AppError('Invalid CSRF token', 403);
    }

    next();
  }

  // í† í° ë¸”ë™ë¦¬ìŠ¤íŠ¸ í™•ì¸
  private static async isTokenBlacklisted(token: string): Promise<boolean> {
    try {
      const redis = new RedisClient();
      const tokenHash = crypto.createHash('sha256').update(token).digest('hex');
      const isBlacklisted = await redis.client.exists(`blacklist:${tokenHash}`);
      return isBlacklisted === 1;
    } catch (error) {
      logger.error('Failed to check token blacklist', error);
      return false; // ì—ëŸ¬ ì‹œ ì•ˆì „í•œ ë°©í–¥ìœ¼ë¡œ ì²˜ë¦¬
    }
  }

  // í† í° ë¸”ë™ë¦¬ìŠ¤íŠ¸ ì¶”ê°€
  static async blacklistToken(token: string, expirationTime: number): Promise<void> {
    try {
      const redis = new RedisClient();
      const tokenHash = crypto.createHash('sha256').update(token).digest('hex');
      await redis.client.setex(`blacklist:${tokenHash}`, expirationTime, 'true');
      
      logger.logSecurityEvent('Token blacklisted', 'low', {
        tokenHash: tokenHash.substring(0, 16) + '...',
        expiration: expirationTime
      });
    } catch (error) {
      logger.error('Failed to blacklist token', error);
    }
  }

  // IP ì£¼ì†Œ ê¸°ë°˜ ì§€ì—­ ì°¨ë‹¨
  static blockGeoLocation(blockedCountries: string[] = []) {
    return async (req: Request, res: Response, next: NextFunction) => {
      try {
        // IP ì§€ì—­ ì¡°íšŒ ì„œë¹„ìŠ¤ (ì˜ˆ: MaxMind GeoIP)
        const geoInfo = await this.getGeoLocation(req.ip);
        
        if (blockedCountries.includes(geoInfo.country)) {
          logger.logSecurityEvent('Blocked geographical access', 'medium', {
            ip: req.ip,
            country: geoInfo.country,
            city: geoInfo.city
          });
          
          throw new AppError('Access denied from this location', 403);
        }
        
        next();
      } catch (error) {
        if (error instanceof AppError) {
          throw error;
        }
        // ì§€ì—­ ì¡°íšŒ ì‹¤íŒ¨ ì‹œ í†µê³¼ (ì„œë¹„ìŠ¤ ê°€ìš©ì„± ìš°ì„ )
        next();
      }
    };
  }

  private static async getGeoLocation(ip: string): Promise<{country: string, city: string}> {
    // ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” MaxMind GeoIP2 ë˜ëŠ” ë‹¤ë¥¸ ì„œë¹„ìŠ¤ ì‚¬ìš©
    return { country: 'US', city: 'Unknown' };
  }

  // ì˜ì‹¬ìŠ¤ëŸ¬ìš´ í™œë™ íƒì§€
  static detectSuspiciousActivity() {
    return async (req: Request, res: Response, next: NextFunction) => {
      const userId = req.user?.id;
      const ip = req.ip;
      const userAgent = req.headers['user-agent'];
      const path = req.path;

      // ì˜ì‹¬ìŠ¤ëŸ¬ìš´ íŒ¨í„´ë“¤
      const suspiciousPatterns = [
        // ì§§ì€ ì‹œê°„ ë‚´ ëŒ€ëŸ‰ ìš”ì²­
        { type: 'rapid_requests', threshold: 50, window: 60 },
        // ë‹¤ì–‘í•œ IPì—ì„œ ë™ì¼ ê³„ì • ì ‘ê·¼
        { type: 'multiple_ips', threshold: 5, window: 300 },
        // ë¹„ì •ìƒì ì¸ ì‹œê°„ëŒ€ ì ‘ê·¼
        { type: 'unusual_hours', threshold: 1, window: 3600 }
      ];

      try {
        for (const pattern of suspiciousPatterns) {
          const isSuspicious = await this.checkSuspiciousPattern(
            pattern, userId, ip, userAgent, path
          );
          
          if (isSuspicious) {
            logger.logSecurityEvent(`Suspicious activity detected: ${pattern.type}`, 'high', {
              userId,
              ip,
              userAgent,
              path,
              pattern: pattern.type
            });

            // ì˜ì‹¬ìŠ¤ëŸ¬ìš´ í™œë™ ì‹œ ì¶”ê°€ ì¸ì¦ ìš”êµ¬ ë˜ëŠ” ê³„ì • ì¼ì‹œ ì ê¸ˆ
            if (pattern.type === 'rapid_requests') {
              throw new AppError('Suspicious activity detected. Please try again later.', 429);
            }
          }
        }

        next();
      } catch (error) {
        if (error instanceof AppError) {
          throw error;
        }
        next(); // íƒì§€ ì‹¤íŒ¨ ì‹œ í†µê³¼
      }
    };
  }

  private static async checkSuspiciousPattern(
    pattern: any, 
    userId: string | undefined, 
    ip: string, 
    userAgent: string | undefined, 
    path: string
  ): Promise<boolean> {
    // Redisë¥¼ ì‚¬ìš©í•œ íŒ¨í„´ íƒì§€ ë¡œì§ êµ¬í˜„
    const redis = new RedisClient();
    const key = `suspicious:${pattern.type}:${userId || ip}`;
    
    const count = await redis.client.incr(key);
    if (count === 1) {
      await redis.client.expire(key, pattern.window);
    }
    
    return count > pattern.threshold;
  }
}

// ë³´ì•ˆ ê´€ë ¨ íƒ€ì… í™•ì¥
declare global {
  namespace Express {
    interface Request {
      user?: {
        id: string;
        email: string;
        role: string;
        senderID?: string;
        permissions: string[];
      };
    }
  }
}

export default SecurityMiddleware;
```

### 9.2 ë°ì´í„° ë³´í˜¸ ë° ì•”í˜¸í™”

#### **<ì½”ë“œ 21>** ë°ì´í„° ì•”í˜¸í™” ì„œë¹„ìŠ¤
```typescript
// src/services/encryptionService.ts
import crypto from 'crypto';
import { promisify } from 'util';
import bcrypt from 'bcryptjs';
import { AppError } from '../utils/AppError';
import { logger } from '../utils/logger';

interface EncryptionResult {
  encryptedData: string;
  iv: string;
  authTag?: string;
}

interface DecryptionParams {
  encryptedData: string;
  iv: string;
  authTag?: string;
}

export class EncryptionService {
  private readonly algorithm = 'aes-256-gcm';
  private readonly keyLength = 32; // 256 bits
  private readonly ivLength = 16; // 128 bits
  private readonly saltRounds = 12;

  // í™˜ê²½ë³€ìˆ˜ì—ì„œ ë§ˆìŠ¤í„° í‚¤ ë¡œë“œ
  private readonly masterKey: Buffer;

  constructor() {
    const key = process.env.ENCRYPTION_KEY;
    if (!key || key.length < 32) {
      throw new Error('ENCRYPTION_KEY must be at least 32 characters');
    }
    
    this.masterKey = crypto.scryptSync(key, 'salt', this.keyLength);
  }

  // ê°œì¸ì‹ë³„ì •ë³´(PII) ì•”í˜¸í™”
  async encryptPII(data: string): Promise<string> {
    try {
      const result = this.encrypt(data);
      
      // ì•”í˜¸í™”ëœ ë°ì´í„°ë¥¼ Base64ë¡œ ì¸ì½”ë”©í•˜ì—¬ ì €ì¥
      const encryptedPackage = {
        data: result.encryptedData,
        iv: result.iv,
        authTag: result.authTag
      };
      
      logger.info('PII data encrypted', {
        dataLength: data.length,
        encryptedLength: result.encryptedData.length
      });
      
      return Buffer.from(JSON.stringify(encryptedPackage)).toString('base64');
      
    } catch (error) {
      logger.error('Failed to encrypt PII data', error);
      throw new AppError('Encryption failed', 500);
    }
  }

  // ê°œì¸ì‹ë³„ì •ë³´(PII) ë³µí˜¸í™”
  async decryptPII(encryptedData: string): Promise<string> {
    try {
      const encryptedPackage = JSON.parse(
        Buffer.from(encryptedData, 'base64').toString('utf8')
      );
      
      const decryptedData = this.decrypt({
        encryptedData: encryptedPackage.data,
        iv: encryptedPackage.iv,
        authTag: encryptedPackage.authTag
      });
      
      logger.info('PII data decrypted', {
        encryptedLength: encryptedData.length,
        decryptedLength: decryptedData.length
      });
      
      return decryptedData;
      
    } catch (error) {
      logger.error('Failed to decrypt PII data', error);
      throw new AppError('Decryption failed', 500);
    }
  }

  // ì¼ë°˜ ë°ì´í„° ì•”í˜¸í™”
  private encrypt(plaintext: string): EncryptionResult {
    const iv = crypto.randomBytes(this.ivLength);
    const cipher = crypto.createCipher(this.algorithm, this.masterKey, iv);
    
    let encrypted = cipher.update(plaintext, 'utf8', 'hex');
    encrypted += cipher.final('hex');
    
    const authTag = cipher.getAuthTag();
    
    return {
      encryptedData: encrypted,
      iv: iv.toString('hex'),
      authTag: authTag.toString('hex')
    };
  }

  // ì¼ë°˜ ë°ì´í„° ë³µí˜¸í™”
  private decrypt(params: DecryptionParams): string {
    const iv = Buffer.from(params.iv, 'hex');
    const authTag = Buffer.from(params.authTag!, 'hex');
    
    const decipher = crypto.createDecipher(this.algorithm, this.masterKey, iv);
    decipher.setAuthTag(authTag);
    
    let decrypted = decipher.update(params.encryptedData, 'hex', 'utf8');
    decrypted += decipher.final('utf8');
    
    return decrypted;
  }

  // íŒ¨ìŠ¤ì›Œë“œ í•´ì‹±
  async hashPassword(password: string): Promise<string> {
    try {
      const salt = await bcrypt.genSalt(this.saltRounds);
      const hashedPassword = await bcrypt.hash(password, salt);
      
      logger.info('Password hashed successfully');
      return hashedPassword;
      
    } catch (error) {
      logger.error('Failed to hash password', error);
      throw new AppError('Password hashing failed', 500);
    }
  }

  // íŒ¨ìŠ¤ì›Œë“œ ê²€ì¦
  async verifyPassword(password: string, hashedPassword: string): Promise<boolean> {
    try {
      const isValid = await bcrypt.compare(password, hashedPassword);
      
      logger.info('Password verification completed', { isValid });
      return isValid;
      
    } catch (error) {
      logger.error('Failed to verify password', error);
      throw new AppError('Password verification failed', 500);
    }
  }

  // í† í° ìƒì„± (ì„¸ì…˜ í† í°, API í‚¤ ë“±)
  generateSecureToken(length: number = 32): string {
    return crypto.randomBytes(length).toString('hex');
  }

  // HMAC ì„œëª… ìƒì„±
  createHMAC(data: string, secret?: string): string {
    const key = secret || process.env.HMAC_SECRET!;
    return crypto.createHmac('sha256', key).update(data).digest('hex');
  }

  // HMAC ì„œëª… ê²€ì¦
  verifyHMAC(data: string, signature: string, secret?: string): boolean {
    const key = secret || process.env.HMAC_SECRET!;
    const expectedSignature = crypto.createHmac('sha256', key).update(data).digest('hex');
    
    // íƒ€ì´ë° ê³µê²© ë°©ì§€ë¥¼ ìœ„í•œ constant-time ë¹„êµ
    return crypto.timingSafeEqual(
      Buffer.from(expectedSignature, 'hex'),
      Buffer.from(signature, 'hex')
    );
  }

  // ë°ì´í„° í•´ì‹± (ì¤‘ë³µ ì œê±°, ë¬´ê²°ì„± ê²€ì‚¬ìš©)
  hashData(data: string, algorithm: 'md5' | 'sha1' | 'sha256' = 'sha256'): string {
    return crypto.createHash(algorithm).update(data).digest('hex');
  }

  // í‚¤ ìœ ë„ í•¨ìˆ˜ (Key Derivation Function)
  deriveKey(password: string, salt: string, iterations: number = 100000): Buffer {
    return crypto.pbkdf2Sync(password, salt, iterations, this.keyLength, 'sha256');
  }

  // ì•ˆì „í•œ ëœë¤ ë¬¸ìì—´ ìƒì„±
  generateRandomString(length: number = 16, charset: string = 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789'): string {
    let result = '';
    const randomBytes = crypto.randomBytes(length);
    
    for (let i = 0; i < length; i++) {
      result += charset[randomBytes[i] % charset.length];
    }
    
    return result;
  }

  // íŒŒì¼ ì•”í˜¸í™”
  async encryptFile(filePath: string, outputPath: string): Promise<void> {
    const fs = require('fs').promises;
    
    try {
      const fileData = await fs.readFile(filePath);
      const encrypted = this.encrypt(fileData.toString('base64'));
      
      const encryptedPackage = {
        data: encrypted.encryptedData,
        iv: encrypted.iv,
        authTag: encrypted.authTag,
        originalName: filePath.split('/').pop(),
        encryptedAt: new Date().toISOString()
      };
      
      await fs.writeFile(outputPath, JSON.stringify(encryptedPackage));
      
      logger.info('File encrypted successfully', {
        originalPath: filePath,
        encryptedPath: outputPath
      });
      
    } catch (error) {
      logger.error('File encryption failed', error);
      throw new AppError('File encryption failed', 500);
    }
  }

  // íŒŒì¼ ë³µí˜¸í™”
  async decryptFile(encryptedFilePath: string, outputPath: string): Promise<void> {
    const fs = require('fs').promises;
    
    try {
      const encryptedContent = await fs.readFile(encryptedFilePath, 'utf8');
      const encryptedPackage = JSON.parse(encryptedContent);
      
      const decryptedData = this.decrypt({
        encryptedData: encryptedPackage.data,
        iv: encryptedPackage.iv,
        authTag: encryptedPackage.authTag
      });
      
      const fileData = Buffer.from(decryptedData, 'base64');
      await fs.writeFile(outputPath, fileData);
      
      logger.info('File decrypted successfully', {
        encryptedPath: encryptedFilePath,
        decryptedPath: outputPath,
        originalName: encryptedPackage.originalName
      });
      
    } catch (error) {
      logger.error('File decryption failed', error);
      throw new AppError('File decryption failed', 500);
    }
  }

  // ê°œì¸ì •ë³´ ë§ˆìŠ¤í‚¹ (ë¡œê·¸ ì¶œë ¥ìš©)
  maskPII(data: string, visibleChars: number = 2): string {
    if (data.length <= visibleChars * 2) {
      return '*'.repeat(data.length);
    }
    
    const start = data.substring(0, visibleChars);
    const end = data.substring(data.length - visibleChars);
    const middle = '*'.repeat(data.length - visibleChars * 2);
    
    return start + middle + end;
  }

  // ë°ì´í„° ë¬´ê²°ì„± ê²€ì¦
  verifyDataIntegrity(data: string, expectedHash: string, algorithm: 'md5' | 'sha1' | 'sha256' = 'sha256'): boolean {
    const actualHash = this.hashData(data, algorithm);
    return crypto.timingSafeEqual(
      Buffer.from(expectedHash, 'hex'),
      Buffer.from(actualHash, 'hex')
    );
  }
}

// ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë‚´ë³´ë‚´ê¸°
export const encryptionService = new EncryptionService();
```

---

## ğŸ”§ 10. ìš´ì˜ ë° ìœ ì§€ë³´ìˆ˜

### 10.1 ì‹œìŠ¤í…œ ê´€ë¦¬ ë„êµ¬

#### **<ì½”ë“œ 22>** ì‹œìŠ¤í…œ ê´€ë¦¬ CLI ë„êµ¬
```typescript
// scripts/admin-cli.ts
#!/usr/bin/env ts-node
import { Command } from 'commander';
import { PrismaClient } from '@prisma/client';
import { encryptionService } from '../src/services/encryptionService';
import { databaseOptimizer } from '../src/utils/databaseOptimization';
import { logger } from '../src/utils/logger';
import { AIModelManager } from '../src/ai/services/modelManager';

const program = new Command();
const prisma = new PrismaClient();
const aiModelManager = new AIModelManager();

program
  .name('icsr-admin')
  .description('ICSR AI System Administration CLI')
  .version('1.0.0');

// ë°ì´í„°ë² ì´ìŠ¤ ê´€ë ¨ ëª…ë ¹ì–´
const dbCommand = program.command('db').description('Database management commands');

dbCommand
  .command('migrate')
  .description('Run database migrations')
  .action(async () => {
    try {
      console.log('ğŸ”„ Running database migrations...');
      // Prisma migrate ì‹¤í–‰
      const { exec } = require('child_process');
      exec('npx prisma migrate deploy', (error: any, stdout: any, stderr: any) => {
        if (error) {
          console.error('âŒ Migration failed:', error);
          process.exit(1);
        }
        console.log('âœ… Migration completed successfully');
        console.log(stdout);
      });
    } catch (error) {
      console.error('âŒ Migration error:', error);
      process.exit(1);
    }
  });

dbCommand
  .command('optimize')
  .description('Optimize database performance')
  .action(async () => {
    try {
      console.log('ğŸ”„ Analyzing database performance...');
      
      const suggestions = await databaseOptimizer.analyzePerformance();
      console.log('ğŸ“Š Performance Analysis Results:');
      suggestions.forEach((suggestion, index) => {
        console.log(`  ${index + 1}. ${suggestion}`);
      });
      
      console.log('\nğŸ”„ Creating recommended indexes...');
      await databaseOptimizer.createRecommendedIndexes();
      
      console.log('ğŸ”„ Updating database statistics...');
      await databaseOptimizer.updateStatistics();
      
      console.log('âœ… Database optimization completed');
      
    } catch (error) {
      console.error('âŒ Database optimization failed:', error);
      process.exit(1);
    }
  });

dbCommand
  .command('backup')
  .option('-o, --output <path>', 'Output file path', `./backup-${new Date().toISOString().split('T')[0]}.sql`)
  .description('Create database backup')
  .action(async (options) => {
    try {
      console.log('ğŸ”„ Creating database backup...');
      
      const { exec } = require('child_process');
      const backupCommand = `pg_dump ${process.env.DATABASE_URL} > ${options.output}`;
      
      exec(backupCommand, (error: any, stdout: any, stderr: any) => {
        if (error) {
          console.error('âŒ Backup failed:', error);
          process.exit(1);
        }
        
        console.log(`âœ… Database backup created: ${options.output}`);
        
        // ë°±ì—… íŒŒì¼ ì•”í˜¸í™”
        encryptionService.encryptFile(options.output, `${options.output}.encrypted`)
          .then(() => {
            console.log(`ğŸ”’ Backup encrypted: ${options.output}.encrypted`);
          })
          .catch((encError) => {
            console.error('âš ï¸  Backup encryption failed:', encError);
          });
      });
      
    } catch (error) {
      console.error('âŒ Backup error:', error);
      process.exit(1);
    }
  });

// AI ëª¨ë¸ ê´€ë ¨ ëª…ë ¹ì–´
const aiCommand = program.command('ai').description('AI model management commands');

aiCommand
  .command('status')  icsr_ai_rules.yml: |
    groups:
    - name: icsr-ai-alerts
      rules:
      - alert: HighErrorRate
        expr: rate(http_requests_total{status=~"5.."}[5m]) > 0.1
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "High error rate detected"
          description: "Error rate is {{ $value | humanizePercentage }} for {{ $labels.job }}"
      
      - alert: AIModelLowAccuracy
        expr: ai_model_accuracy < 0.75
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "AI model accuracy is low"
          description: "AI model {{ $labels.model_name }} accuracy is {{ $value | humanizePercentage }}"
      
      - alert: DatabaseConnectionHigh
        expr: pg_stat_activity_count > 80
        for: 3m
        labels:
          severity: warning
        annotations:
          summary: "High database connections"
          description: "PostgreSQL has {{ $value }} active connections"
      
      - alert: PodCrashLooping
        expr: rate(kube_pod_container_status_restarts_total[15m]) > 0
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Pod is crash looping"
          description: "Pod {{ $labels.pod }} is restarting frequently"
      
      - alert: MemoryUsageHigh
        expr: container_memory_usage_bytes / container_spec_memory_limit_bytes > 0.9
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High memory usage"
          description: "Container {{ $labels.container }} memory usage is {{ $value | humanizePercentage }}"
```

#### **<ì½”ë“œ 16>** ì• í”Œë¦¬ì¼€ì´ì…˜ ë¡œê¹… êµ¬ì„±
```typescript
// src/utils/logger.ts
import winston from 'winston';
import 'winston-daily-rotate-file';

interface LogContext {
  userId?: string;
  caseId?: string;
  requestId?: string;
  action?: string;
  duration?: number;
}

class Logger {
  private logger: winston.Logger;

  constructor() {
    this.logger = winston.createLogger({
      level: process.env.LOG_LEVEL || 'info',
      format: winston.format.combine(
        winston.format.timestamp(),
        winston.format.errors({ stack: true }),
        winston.format.json(),
        winston.format.printf(({ timestamp, level, message, ...meta }) => {
          return JSON.stringify({
            '@timestamp': timestamp,
            level,
            message,
            service: 'icsr-ai-backend',
            version: process.env.APP_VERSION || '1.0.0',
            environment: process.env.NODE_ENV || 'development',
            ...meta
          });
        })
      ),
      transports: [
        // Console output
        new winston.transports.Console({
          format: winston.format.combine(
            winston.format.colorize(),
            winston.format.simple()
          )
        }),

        // File output
        new winston.transports.DailyRotateFile({
          filename: 'logs/application-%DATE%.log',
          datePattern: 'YYYY-MM-DD',
          zippedArchive: true,
          maxSize: '100m',
          maxFiles: '30d'
        }),

        // Error file output
        new winston.transports.DailyRotateFile({
          filename: 'logs/error-%DATE%.log',
          datePattern: 'YYYY-MM-DD',
          level: 'error',
          zippedArchive: true,
          maxSize: '100m',
          maxFiles: '30d'
        })
      ]
    });

    // Elasticsearch transport in production
    if (process.env.NODE_ENV === 'production') {
      const { ElasticsearchTransport } = require('winston-elasticsearch');
      
      this.logger.add(new ElasticsearchTransport({
        level: 'info',
        clientOpts: {
          node: process.env.ELASTICSEARCH_URL || 'http://elasticsearch:9200'
        },
        index: 'icsr-ai-logs',
        indexTemplate: {
          name: 'icsr-ai-template',
          patterns: ['icsr-ai-logs-*'],
          settings: {
            number_of_shards: 1,
            number_of_replicas: 1
          },
          mappings: {
            properties: {
              '@timestamp': { type: 'date' },
              level: { type: 'keyword' },
              message: { type: 'text' },
              service: { type: 'keyword' },
              userId: { type: 'keyword' },
              caseId: { type: 'keyword' },
              requestId: { type: 'keyword' },
              action: { type: 'keyword' },
              duration: { type: 'long' },
              stack: { type: 'text' }
            }
          }
        }
      }));
    }
  }

  info(message: string, context?: LogContext): void {
    this.logger.info(message, context);
  }

  error(message: string, error?: Error | any, context?: LogContext): void {
    const errorInfo = error instanceof Error ? {
      stack: error.stack,
      name: error.name
    } : error;

    this.logger.error(message, {
      error: errorInfo,
      ...context
    });
  }

  warn(message: string, context?: LogContext): void {
    this.logger.warn(message, context);
  }

  debug(message: string, context?: LogContext): void {
    this.logger.debug(message, context);
  }

  // ì„±ëŠ¥ ë¡œê¹…
  logPerformance(action: string, duration: number, context?: LogContext): void {
    this.info(`Performance: ${action} completed`, {
      action,
      duration,
      ...context
    });
  }

  // ë¹„ì¦ˆë‹ˆìŠ¤ ì´ë²¤íŠ¸ ë¡œê¹…
  logBusinessEvent(event: string, context?: LogContext): void {
    this.info(`Business Event: ${event}`, {
      event_type: 'business',
      event_name: event,
      ...context
    });
  }

  // AI ì²˜ë¦¬ ë¡œê¹…
  logAIProcessing(modelName: string, accuracy: number, processingTime: number, context?: LogContext): void {
    this.info('AI Processing completed', {
      event_type: 'ai_processing',
      model_name: modelName,
      accuracy,
      processing_time: processingTime,
      ...context
    });
  }

  // ë³´ì•ˆ ì´ë²¤íŠ¸ ë¡œê¹…
  logSecurityEvent(event: string, severity: 'low' | 'medium' | 'high' | 'critical', context?: LogContext): void {
    this.warn(`Security Event: ${event}`, {
      event_type: 'security',
      event_name: event,
      severity,
      ...context
    });
  }
}

export const logger = new Logger();
```

---

## ğŸ“Š 8. ì„±ëŠ¥ ìµœì í™” ë° ëª¨ë‹ˆí„°ë§

### 8.1 ì„±ëŠ¥ ìµœì í™” ì „ëµ

#### **<í‘œ 10>** ì„±ëŠ¥ ìµœì í™” ì²´í¬ë¦¬ìŠ¤íŠ¸
| ì˜ì—­ | ìµœì í™” ê¸°ë²• | êµ¬í˜„ ìƒíƒœ | ì˜ˆìƒ íš¨ê³¼ |
|------|------------|-----------|-----------|
| **Frontend** | ì½”ë“œ ìŠ¤í”Œë¦¬íŒ… | âœ… ì™„ë£Œ | ì´ˆê¸° ë¡œë”© 40% ë‹¨ì¶• |
| **Frontend** | ì´ë¯¸ì§€ ìµœì í™” | âœ… ì™„ë£Œ | ë²ˆë“¤ í¬ê¸° 30% ê°ì†Œ |
| **Frontend** | ë©”ëª¨ì´ì œì´ì…˜ | âœ… ì™„ë£Œ | ë Œë”ë§ ì„±ëŠ¥ 25% í–¥ìƒ |
| **Backend** | API ì‘ë‹µ ìºì‹± | âœ… ì™„ë£Œ | API ì‘ë‹µ ì‹œê°„ 60% ë‹¨ì¶• |
| **Backend** | ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™” | âœ… ì™„ë£Œ | ì¿¼ë¦¬ ì„±ëŠ¥ 50% í–¥ìƒ |
| **Backend** | ì»¤ë„¥ì…˜ í’€ë§ | âœ… ì™„ë£Œ | ë™ì‹œ ì²˜ë¦¬ëŸ‰ 200% ì¦ê°€ |
| **AI Engine** | ëª¨ë¸ ì–‘ìí™” | ğŸ”„ ì§„í–‰ì¤‘ | ì¶”ë¡  ì‹œê°„ 70% ë‹¨ì¶• |
| **AI Engine** | ë°°ì¹˜ ì²˜ë¦¬ | âœ… ì™„ë£Œ | ì²˜ë¦¬ëŸ‰ 300% ì¦ê°€ |
| **Infrastructure** | CDN ì ìš© | âœ… ì™„ë£Œ | ë¡œë”© ì‹œê°„ 50% ë‹¨ì¶• |

#### **<ì½”ë“œ 17>** ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë¯¸ë“¤ì›¨ì–´
```typescript
// src/middleware/performanceMiddleware.ts
import { Request, Response, NextFunction } from 'express';
import { performance } from 'perf_hooks';
import { logger } from '../utils/logger';
import { prometheusMetrics } from '../utils/metrics';

interface PerformanceData {
  requestId: string;
  method: string;
  path: string;
  statusCode: number;
  responseTime: number;
  memoryUsage: NodeJS.MemoryUsage;
  cpuUsage: NodeJS.CpuUsage;
}

export const performanceMiddleware = (req: Request, res: Response, next: NextFunction) => {
  const startTime = performance.now();
  const startCpuUsage = process.cpuUsage();
  const requestId = req.headers['x-request-id'] as string || generateRequestId();

  // Request ID ì„¤ì •
  req.requestId = requestId;
  res.setHeader('X-Request-ID', requestId);

  // Response ì™„ë£Œ í›„ ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
  res.on('finish', () => {
    const endTime = performance.now();
    const responseTime = endTime - startTime;
    const endCpuUsage = process.cpuUsage(startCpuUsage);
    const memoryUsage = process.memoryUsage();

    const performanceData: PerformanceData = {
      requestId,
      method: req.method,
      path: req.route?.path || req.path,
      statusCode: res.statusCode,
      responseTime: Math.round(responseTime * 100) / 100, // ì†Œìˆ˜ì  2ìë¦¬
      memoryUsage,
      cpuUsage: endCpuUsage
    };

    // ì„±ëŠ¥ ë¡œê·¸
    logger.logPerformance(`${req.method} ${req.path}`, responseTime, {
      requestId,
      statusCode: res.statusCode,
      memoryUsed: Math.round(memoryUsage.heapUsed / 1024 / 1024), // MB
      cpuUser: endCpuUsage.user,
      cpuSystem: endCpuUsage.system
    });

    // Prometheus ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
    prometheusMetrics.httpRequestDuration
      .labels(req.method, req.route?.path || req.path, res.statusCode.toString())
      .observe(responseTime / 1000); // ì´ˆ ë‹¨ìœ„ë¡œ ë³€í™˜

    prometheusMetrics.httpRequestsTotal
      .labels(req.method, req.route?.path || req.path, res.statusCode.toString())
      .inc();

    // ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë©”íŠ¸ë¦­
    prometheusMetrics.memoryUsage.set(memoryUsage.heapUsed);

    // ëŠë¦° ìš”ì²­ ì•Œë¦¼ (5ì´ˆ ì´ìƒ)
    if (responseTime > 5000) {
      logger.warn('Slow request detected', {
        requestId,
        method: req.method,
        path: req.path,
        responseTime,
        statusCode: res.statusCode
      });
    }

    // ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë†’ì€ ê²½ìš° ì•Œë¦¼ (500MB ì´ìƒ)
    if (memoryUsage.heapUsed > 500 * 1024 * 1024) {
      logger.warn('High memory usage detected', {
        requestId,
        heapUsed: Math.round(memoryUsage.heapUsed / 1024 / 1024),
        heapTotal: Math.round(memoryUsage.heapTotal / 1024 / 1024)
      });
    }
  });

  next();
};

// ìš”ì²­ ID ìƒì„±
function generateRequestId(): string {
  return `req_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;
}

// Request íƒ€ì… í™•ì¥
declare global {
  namespace Express {
    interface Request {
      requestId: string;
    }
  }
}
```

#### **<ì½”ë“œ 18>** ë°ì´í„°ë² ì´ìŠ¤ ì„±ëŠ¥ ìµœì í™”
```typescript
// src/utils/databaseOptimization.ts
import { PrismaClient } from '@prisma/client';
import { logger } from './logger';

class DatabaseOptimizer {
  constructor(private prisma: PrismaClient) {
    this.setupQueryLogging();
    this.setupConnectionPooling();
  }

  private setupQueryLogging(): void {
    // ìŠ¬ë¡œìš° ì¿¼ë¦¬ ë¡œê¹…
    this.prisma.$use(async (params, next) => {
      const start = Date.now();
      const result = await next(params);
      const duration = Date.now() - start;

      // 1ì´ˆ ì´ìƒ ê±¸ë¦° ì¿¼ë¦¬ ë¡œê¹…
      if (duration > 1000) {
        logger.warn('Slow query detected', {
          model: params.model,
          action: params.action,
          duration,
          args: JSON.stringify(params.args, null, 2)
        });
      }

      return result;
    });
  }

  private setupConnectionPooling(): void {
    // ì»¤ë„¥ì…˜ í’€ ì„¤ì •ì€ DATABASE_URLì—ì„œ ì„¤ì •
    // postgresql://user:pass@host:5432/db?connection_limit=20&pool_timeout=20
  }

  // ì¸ë±ìŠ¤ ìµœì í™” ì œì•ˆ
  async analyzePerformance(): Promise<string[]> {
    const suggestions: string[] = [];

    try {
      // ìì£¼ ì‚¬ìš©ë˜ëŠ” ì¿¼ë¦¬ íŒ¨í„´ ë¶„ì„
      const caseQueries = await this.prisma.$queryRaw`
        SELECT schemaname, tablename, attname, n_distinct, correlation
        FROM pg_stats 
        WHERE tablename IN ('cases', 'patients', 'drugs', 'events')
        ORDER BY n_distinct DESC;
      `;

      // ëŠë¦° ì¿¼ë¦¬ ë¶„ì„
      const slowQueries = await this.prisma.$queryRaw`
        SELECT query, mean_time, calls, total_time
        FROM pg_stat_statements 
        WHERE mean_time > 1000
        ORDER BY mean_time DESC
        LIMIT 10;
      `;

      if (Array.isArray(caseQueries)) {
        suggestions.push(`ë¶„ì„ëœ í…Œì´ë¸”: ${caseQueries.length}ê°œ`);
      }

      if (Array.isArray(slowQueries) && slowQueries.length > 0) {
        suggestions.push(`ëŠë¦° ì¿¼ë¦¬ ë°œê²¬: ${slowQueries.length}ê°œ`);
        suggestions.push('ì¸ë±ìŠ¤ ì¶”ê°€ë¥¼ ê³ ë ¤í•´ë³´ì„¸ìš”.');
      }

    } catch (error) {
      logger.error('Database performance analysis failed', error);
      suggestions.push('ì„±ëŠ¥ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ');
    }

    return suggestions;
  }

  // ê¶Œì¥ ì¸ë±ìŠ¤ ìƒì„±
  async createRecommendedIndexes(): Promise<void> {
    const indexes = [
      // ì¼€ì´ìŠ¤ ê²€ìƒ‰ ìµœì í™”
      'CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_cases_status_created ON cases (status, created_at);',
      'CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_cases_sender_updated ON cases (sender_id, updated_at);',
      
      // í™˜ì ì •ë³´ ê²€ìƒ‰ ìµœì í™”
      'CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_patients_initials ON patients (initials);',
      
      // ì˜ì•½í’ˆ ê²€ìƒ‰ ìµœì í™”
      'CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_drugs_name ON drugs USING gin (name gin_trgm_ops);',
      'CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_drugs_meddra ON drugs (meddra_code);',
      
      // ì´ìƒì‚¬ë¡€ ê²€ìƒ‰ ìµœì í™”
      'CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_events_verbatim ON events USING gin (verbatim_term gin_trgm_ops);',
      'CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_events_pt_code ON events (pt_code);',
      
      // AI ì˜ˆì¸¡ ê²€ìƒ‰ ìµœì í™”
      'CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_ai_predictions_case_confidence ON ai_predictions (case_id, confidence);',
      
      // ê°ì‚¬ ë¡œê·¸ ìµœì í™”
      'CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_audit_logs_timestamp ON audit_logs (timestamp);',
      'CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_audit_logs_user_action ON audit_logs (user_id, action);'
    ];

    for (const indexQuery of indexes) {
      try {
        await this.prisma.$executeRawUnsafe(indexQuery);
        logger.info('Index created successfully', { query: indexQuery });
      } catch (error) {
        logger.error('Failed to create index', error, { query: indexQuery });
      }
    }
  }

  // ë°ì´í„°ë² ì´ìŠ¤ í†µê³„ ì—…ë°ì´íŠ¸
  async updateStatistics(): Promise<void> {
    try {
      await this.prisma.$executeRawUnsafe('ANALYZE;');
      logger.info('Database statistics updated');
    } catch (error) {
      logger.error('Failed to update database statistics', error);
    }
  }

  // ì—°ê²° ìƒíƒœ ëª¨ë‹ˆí„°ë§
  async monitorConnections(): Promise<{
    active: number;
    idle: number;
    total: number;
  }> {
    try {
      const result = await this.prisma.$queryRaw<Array<{
        state: string;
        count: bigint;
      }>>`
        SELECT state, COUNT(*) as count
        FROM pg_stat_activity 
        WHERE datname = current_database()
        GROUP BY state;
      `;

      const connections = {
        active: 0,
        idle: 0,
        total: 0
      };

      result.forEach(row => {
        const count = Number(row.count);
        if (row.state === 'active') {
          connections.active = count;
        } else if (row.state === 'idle') {
          connections.idle = count;
        }
        connections.total += count;
      });

      return connections;
    } catch (error) {
      logger.error('Failed to monitor database connections', error);
      return { active: 0, idle: 0, total: 0 };
    }
  }
}

export const databaseOptimizer = new DatabaseOptimizer(new PrismaClient());
```

### 8.2 AI ëª¨ë¸ ì„±ëŠ¥ ìµœì í™”

#### **<ì½”ë“œ 19>** AI ëª¨ë¸ ìµœì í™” ë° ëª¨ë‹ˆí„°ë§
```python
# src/ai/utils/model_optimizer.py
import torch
import torch.nn as nn
from torch.quantization import quantize_dynamic
import time
import logging
from typing import Dict, Any, List
import psutil
import GPUtil
from dataclasses import dataclass

logger = logging.getLogger(__name__)

@dataclass
class ModelPerformanceMetrics:
    """ëª¨ë¸ ì„±ëŠ¥ ë©”íŠ¸ë¦­"""
    inference_time: float
    memory_usage: float
    gpu_utilization: float
    accuracy: float
    throughput: float  # requests per second

class AIModelOptimizer:
    """AI ëª¨ë¸ ì„±ëŠ¥ ìµœì í™” í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.optimization_cache = {}
        
    def optimize_model_for_inference(self, model: nn.Module, optimization_level: str = 'medium') -> nn.Module:
        """
        ì¶”ë¡ ì„ ìœ„í•œ ëª¨ë¸ ìµœì í™”
        
        Args:
            model: ìµœì í™”í•  PyTorch ëª¨ë¸
            optimization_level: 'low', 'medium', 'high'
            
        Returns:
            ìµœì í™”ëœ ëª¨ë¸
        """
        try:
            logger.info(f"Starting model optimization (level: {optimization_level})")
            
            # í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •
            model.eval()
            
            if optimization_level in ['medium', 'high']:
                # ë™ì  ì–‘ìí™” ì ìš©
                model = self._apply_dynamic_quantization(model)
                
            if optimization_level == 'high':
                # ë” ê°•ë ¥í•œ ìµœì í™” ì ìš©
                model = self._apply_advanced_optimization(model)
            
            # JIT ì»´íŒŒì¼ (ì¶”ë¡  ì†ë„ í–¥ìƒ)
            model = torch.jit.script(model)
            
            logger.info("Model optimization completed")
            return model
            
        except Exception as e:
            logger.error(f"Model optimization failed: {str(e)}")
            return model  # ìµœì í™” ì‹¤íŒ¨ ì‹œ ì›ë³¸ ëª¨ë¸ ë°˜í™˜

    def _apply_dynamic_quantization(self, model: nn.Module) -> nn.Module:
        """ë™ì  ì–‘ìí™” ì ìš©"""
        try:
            quantized_model = quantize_dynamic(
                model,
                {nn.Linear, nn.LSTM, nn.GRU},  # ì–‘ìí™”í•  ë ˆì´ì–´ íƒ€ì…
                dtype=torch.qint8
            )
            
            logger.info("Dynamic quantization applied")
            return quantized_model
            
        except Exception as e:
            logger.warning(f"Dynamic quantization failed: {str(e)}")
            return model

    def _apply_advanced_optimization(self, model: nn.Module) -> nn.Module:
        """ê³ ê¸‰ ìµœì í™” ê¸°ë²• ì ìš©"""
        try:
            # ê·¸ë˜ë””ì–¸íŠ¸ ë¹„í™œì„±í™” (ì¶”ë¡  ì‹œ ë©”ëª¨ë¦¬ ì ˆì•½)
            for param in model.parameters():
                param.requires_grad = False
            
            # í˜¼í•© ì •ë°€ë„ ìµœì í™” (GPUì—ì„œ)
            if self.device.type == 'cuda':
                model = model.half()  # FP16 ì‚¬ìš©
                
            logger.info("Advanced optimization applied")
            return model
            
        except Exception as e:
            logger.warning(f"Advanced optimization failed: {str(e)}")
            return model

    async def benchmark_model_performance(
        self, 
        model: nn.Module, 
        sample_inputs: List[torch.Tensor],
        num_iterations: int = 100
    ) -> ModelPerformanceMetrics:
        """
        ëª¨ë¸ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹
        
        Args:
            model: ë²¤ì¹˜ë§ˆí‚¹í•  ëª¨ë¸
            sample_inputs: í…ŒìŠ¤íŠ¸ìš© ì…ë ¥ ë°ì´í„°
            num_iterations: ë°˜ë³µ íšŸìˆ˜
            
        Returns:
            ì„±ëŠ¥ ë©”íŠ¸ë¦­
        """
        logger.info(f"Starting performance benchmark ({num_iterations} iterations)")
        
        model.eval()
        inference_times = []
        
        # GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì • ì‹œì‘
        if torch.cuda.is_available():
            torch.cuda.reset_peak_memory_stats()
            initial_gpu_memory = torch.cuda.memory_allocated()
        
        # CPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì • ì‹œì‘
        initial_cpu_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB
        
        # ì›Œë°ì—… (ì²« ëª‡ ë²ˆì˜ ì¶”ë¡ ì€ ì œì™¸)
        with torch.no_grad():
            for _ in range(10):
                _ = model(sample_inputs[0])
        
        # ì‹¤ì œ ë²¤ì¹˜ë§ˆí¬
        with torch.no_grad():
            for i in range(num_iterations):
                input_tensor = sample_inputs[i % len(sample_inputs)]
                
                start_time = time.time()
                _ = model(input_tensor)
                end_time = time.time()
                
                inference_times.append(end_time - start_time)
                
                # GPU ë™ê¸°í™” (ì •í™•í•œ ì‹œê°„ ì¸¡ì •ì„ ìœ„í•´)
                if torch.cuda.is_available():
                    torch.cuda.synchronize()
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚°
        avg_inference_time = sum(inference_times) / len(inference_times)
        throughput = 1.0 / avg_inference_time
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê³„ì‚°
        if torch.cuda.is_available():
            gpu_memory_used = (torch.cuda.max_memory_allocated() - initial_gpu_memory) / 1024 / 1024  # MB
            gpu_utilization = self._get_gpu_utilization()
        else:
            gpu_memory_used = 0
            gpu_utilization = 0
            
        final_cpu_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB
        cpu_memory_used = final_cpu_memory - initial_cpu_memory
        
        metrics = ModelPerformanceMetrics(
            inference_time=avg_inference_time * 1000,  # milliseconds
            memory_usage=max(gpu_memory_used, cpu_memory_used),
            gpu_utilization=gpu_utilization,
            accuracy=0.0,  # ë³„ë„ í‰ê°€ í•„ìš”
            throughput=throughput
        )
        
        logger.info(f"Benchmark completed - Avg inference time: {metrics.inference_time:.2f}ms, "
                   f"Throughput: {metrics.throughput:.2f} req/s")
        
        return metrics

    def _get_gpu_utilization(self) -> float:
        """GPU ì‚¬ìš©ë¥  ì¡°íšŒ"""
        try:
            gpus = GPUtil.getGPUs()
            if gpus:
                return gpus[0].load * 100  # ì²« ë²ˆì§¸ GPU ì‚¬ìš©ë¥ 
            return 0.0
        except Exception:
            return 0.0

    async def optimize_batch_processing(
        self, 
        model: nn.Module,
        batch_sizes: List[int] = [1, 4, 8, 16, 32]
    ) -> int:
        """
        ìµœì  ë°°ì¹˜ í¬ê¸° ì°¾ê¸°
        
        Args:
            model: ëª¨ë¸
            batch_sizes: í…ŒìŠ¤íŠ¸í•  ë°°ì¹˜ í¬ê¸° ë¦¬ìŠ¤íŠ¸
            
        Returns:
            ìµœì  ë°°ì¹˜ í¬ê¸°
        """
        logger.info("Optimizing batch size for processing")
        
        best_batch_size = 1
        best_throughput = 0.0
        
        for batch_size in batch_sizes:
            try:
                # ìƒ˜í”Œ ë°°ì¹˜ ìƒì„± (ì‹¤ì œ ì…ë ¥ í¬ê¸°ì— ë§ê²Œ ì¡°ì • í•„ìš”)
                sample_batch = torch.randn(batch_size, 512)  # ì˜ˆì‹œ í¬ê¸°
                
                # ë©”ëª¨ë¦¬ ì²´í¬
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                    
                start_time = time.time()
                
                with torch.no_grad():
                    for _ in range(10):  # 10ë²ˆ ë°˜ë³µ í…ŒìŠ¤íŠ¸
                        _ = model(sample_batch)
                        
                if torch.cuda.is_available():
                    torch.cuda.synchronize()
                    
                end_time = time.time()
                
                # ì²˜ë¦¬ëŸ‰ ê³„ì‚° (samples per second)
                total_samples = batch_size * 10
                total_time = end_time - start_time
                throughput = total_samples / total_time
                
                logger.info(f"Batch size {batch_size}: {throughput:.2f} samples/sec")
                
                if throughput > best_throughput:
                    best_throughput = throughput
                    best_batch_size = batch_size
                    
            except RuntimeError as e:
                if "out of memory" in str(e):
                    logger.warning(f"OOM at batch size {batch_size}")
                    break  # ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ ì¤‘ë‹¨
                else:
                    logger.error(f"Error testing batch size {batch_size}: {str(e)}")
        
        logger.info(f"Optimal batch size: {best_batch_size} (throughput: {best_throughput:.2f} samples/sec)")
        return best_batch_size

    def monitor_model_drift(
        self,
        current_accuracy: float,
        baseline_accuracy: float,
        drift_threshold: float = 0.05
    ) -> Dict[str, Any]:
        """
        ëª¨ë¸ ë“œë¦¬í”„íŠ¸ ëª¨ë‹ˆí„°ë§
        
        Args:
            current_accuracy: í˜„ì¬ ì •í™•ë„
            baseline_accuracy: ê¸°ì¤€ ì •í™•ë„
            drift_threshold: ë“œë¦¬í”„íŠ¸ ì„ê³„ê°’
            
        Returns:
            ë“œë¦¬í”„íŠ¸ ë¶„ì„ ê²°ê³¼
        """
        accuracy_drop = baseline_accuracy - current_accuracy
        is_drifted = accuracy_drop > drift_threshold
        
        drift_info = {
            'is_drifted': is_drifted,
            'accuracy_drop': accuracy_drop,
            'current_accuracy': current_accuracy,
            'baseline_accuracy': baseline_accuracy,
            'drift_threshold': drift_threshold,
            'severity': self._calculate_drift_severity(accuracy_drop, drift_threshold)
        }
        
        if is_drifted:
            logger.warning(f"Model drift detected! Accuracy dropped by {accuracy_drop:.3f}")
        
        return drift_info

    def _calculate_drift_severity(self, accuracy_drop: float, threshold: float) -> str:
        """ë“œë¦¬í”„íŠ¸ ì‹¬ê°ë„ ê³„ì‚°"""
        if accuracy_drop <= threshold:
            return 'none'
        elif accuracy_drop <= threshold * 2:
            return 'low'
        elif accuracy_drop <= threshold * 4:
            return 'medium'
        else:
            return 'high'

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
model_optimizer = AIModelOptimizer()
```

---

## ğŸ”’ 9. ë³´ì•ˆ ë° ì»´í”Œë¼ì´ì–¸ìŠ¤

### 9.1 ë³´ì•ˆ ì•„í‚¤í…ì²˜

#### **<í‘œ 11>** ë³´ì•ˆ ìš”êµ¬ì‚¬í•­ ë§¤íŠ¸ë¦­ìŠ¤
| ë³´ì•ˆ ì˜ì—­ | ìš”êµ¬ì‚¬í•­ | êµ¬í˜„ ë°©ë²• | ì¤€ìˆ˜ í‘œì¤€ |
|----------|----------|-----------|-----------|
| **ì¸ì¦/ì¸ê°€** | ë‹¤ë‹¨ê³„ ì¸ì¦ | OAuth 2.0 + TOTP | ISO 27001 |
| **ë°ì´í„° ì•”# AI ê¸°ë°˜ ì•½ë¬¼ê°ì‹œ ì‹œìŠ¤í…œ ì†Œí”„íŠ¸ì›¨ì–´ ì œì‘ ì‚¬ì–‘ì„œ

## ğŸ“‹ ë¬¸ì„œ ì •ë³´

- **í”„ë¡œì íŠ¸ëª…**: ICSR-AI (Individual Case Safety Report - AI)
- **ë¬¸ì„œ ë²„ì „**: v2.0
- **ì‘ì„±ì¼**: 2025ë…„ 9ì›” 12ì¼
- **ìŠ¹ì¸ì**: ê¸°ìˆ ì´ì‚¬, PV ì±…ì„ì
- **ë¬¸ì„œ ìœ í˜•**: ì†Œí”„íŠ¸ì›¨ì–´ ìš”êµ¬ì‚¬í•­ ëª…ì„¸ì„œ (SRS)

---

## ğŸ¯ 1. í”„ë¡œì íŠ¸ ê°œìš”

### 1.1 í”„ë¡œì íŠ¸ ëª©í‘œ
**ì•½ë¬¼ê°ì‹œì—…ê³„ì˜ ICSR ì²˜ë¦¬ ê³¼ì •ì„ AIë¡œ ìë™í™”í•˜ì—¬ ì—…ë¬´ íš¨ìœ¨ì„± í–¥ìƒ ë° ê·œì œ ì¤€ìˆ˜ ë³´ì¥**

### 1.2 ì£¼ìš” ì„±ê³¼ ëª©í‘œ
- **<í‘œ 1>** ICSR ì²˜ë¦¬ ì‹œê°„ 75% ë‹¨ì¶• (60ë¶„ â†’ 15ë¶„)
- **<í‘œ 2>** AI ìë™í™”ìœ¨ 85% ë‹¬ì„±
- **<í‘œ 3>** ìš´ì˜ë¹„ìš© 40% ì ˆê°
- **<í‘œ 4>** ì²˜ë¦¬ ìš©ëŸ‰ 300% ì¦ê°€

### 1.3 í”„ë¡œì íŠ¸ ë²”ìœ„

#### í¬í•¨ ì‚¬í•­ (In-Scope)
âœ… ICSR ì¼€ì´ìŠ¤ ê´€ë¦¬ ì‹œìŠ¤í…œ  
âœ… AI ê¸°ë°˜ ë°ì´í„° ì¶”ì¶œ ë° ë¶„ë¥˜  
âœ… MedDRA ìë™ ì½”ë”©  
âœ… E2B(R3) XML ìƒì„± ë° ê²€ì¦  
âœ… ê·œì œê¸°ê´€ ìë™ ì œì¶œ  
âœ… ëŒ€ì‹œë³´ë“œ ë° ë¦¬í¬íŒ…  

#### ì œì™¸ ì‚¬í•­ (Out-of-Scope)  
âŒ ê¸°ì¡´ ë ˆê±°ì‹œ ì‹œìŠ¤í…œ ë§ˆì´ê·¸ë ˆì´ì…˜  
âŒ í•˜ë“œì›¨ì–´ êµ¬ë§¤ ë° ì„¤ì¹˜  
âŒ íƒ€ì‚¬ ì‹œìŠ¤í…œê³¼ì˜ ì§ì ‘ ì—°ë™  

---

## ğŸ—ï¸ 2. ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜

### 2.1 ì „ì²´ ì‹œìŠ¤í…œ êµ¬ì¡°ë„

```mermaid
graph TB
    A[Web Browser] --> B[Load Balancer]
    B --> C[React Frontend]
    C --> D[API Gateway]
    D --> E[Microservices]
    E --> F[AI Engine]
    E --> G[Database Cluster]
    E --> H[External APIs]
    
    subgraph "Frontend Layer"
        C
    end
    
    subgraph "Backend Layer"
        D
        E
        I[Redis Cache]
        J[Message Queue]
    end
    
    subgraph "AI/ML Layer"
        F
        K[Model Registry]
        L[Training Pipeline]
    end
    
    subgraph "Data Layer"
        G
        M[File Storage]
    end
```

### 2.2 ê¸°ìˆ  ìŠ¤íƒ ë§¤íŠ¸ë¦­ìŠ¤

#### **<í‘œ 5>** Front-End ê¸°ìˆ  ìŠ¤íƒ
| êµ¬ë¶„ | ê¸°ìˆ  | ë²„ì „ | ìš©ë„ |
|------|------|------|------|
| Framework | React | 18.2+ | UI í”„ë ˆì„ì›Œí¬ |
| Language | TypeScript | 5.0+ | íƒ€ì… ì•ˆì •ì„± |
| State Management | Redux Toolkit | 1.9+ | ì „ì—­ ìƒíƒœ ê´€ë¦¬ |
| UI Library | Material-UI | 5.14+ | UI ì»´í¬ë„ŒíŠ¸ |
| Build Tool | Vite | 4.4+ | ë¹Œë“œ ë„êµ¬ |
| Testing | Jest + RTL | Latest | í…ŒìŠ¤íŒ… |

#### **<í‘œ 6>** Back-End ê¸°ìˆ  ìŠ¤íƒ
| êµ¬ë¶„ | ê¸°ìˆ  | ë²„ì „ | ìš©ë„ |
|------|------|------|------|
| Runtime | Node.js | 20.x LTS | ì„œë²„ ëŸ°íƒ€ì„ |
| Framework | Express.js | 4.18+ | ì›¹ í”„ë ˆì„ì›Œí¬ |
| Language | TypeScript | 5.0+ | íƒ€ì… ì•ˆì •ì„± |
| Database | PostgreSQL | 16.x | ë©”ì¸ ë°ì´í„°ë² ì´ìŠ¤ |
| Cache | Redis | 7.x | ìºì‹± ë° ì„¸ì…˜ |
| ORM | Prisma | 5.x | ë°ì´í„°ë² ì´ìŠ¤ ORM |

#### **<í‘œ 7>** AI/ML ê¸°ìˆ  ìŠ¤íƒ
| êµ¬ë¶„ | ê¸°ìˆ  | ë²„ì „ | ìš©ë„ |
|------|------|------|------|
| Runtime | Python | 3.11+ | AI/ML ëŸ°íƒ€ì„ |
| Framework | FastAPI | 0.103+ | API ì„œë²„ |
| ML Library | scikit-learn | 1.3+ | ì „í†µì  ML |
| NLP | Transformers | 4.33+ | ìì—°ì–´ ì²˜ë¦¬ |
| Model Serving | MLflow | 2.6+ | ëª¨ë¸ ê´€ë¦¬ |
| GPU Support | CUDA | 12.0+ | GPU ê°€ì† |

---

## ğŸ“± 3. Front-End ìƒì„¸ ì„¤ê³„

### 3.1 ë””ë ‰í† ë¦¬ êµ¬ì¡°

```
src/
â”œâ”€â”€ components/          # ì¬ì‚¬ìš© ê°€ëŠ¥í•œ ì»´í¬ë„ŒíŠ¸
â”‚   â”œâ”€â”€ common/         # ê³µí†µ ì»´í¬ë„ŒíŠ¸
â”‚   â”œâ”€â”€ forms/          # í¼ ì»´í¬ë„ŒíŠ¸
â”‚   â””â”€â”€ charts/         # ì°¨íŠ¸ ì»´í¬ë„ŒíŠ¸
â”œâ”€â”€ pages/              # í˜ì´ì§€ ì»´í¬ë„ŒíŠ¸
â”‚   â”œâ”€â”€ dashboard/      # ëŒ€ì‹œë³´ë“œ
â”‚   â”œâ”€â”€ cases/          # ICSR ì¼€ì´ìŠ¤ ê´€ë¦¬
â”‚   â”œâ”€â”€ meddra/         # MedDRA ê´€ë¦¬
â”‚   â””â”€â”€ reports/        # ë¦¬í¬íŠ¸
â”œâ”€â”€ hooks/              # Custom React Hooks
â”œâ”€â”€ store/              # Redux Store
â”œâ”€â”€ services/           # API ì„œë¹„ìŠ¤
â”œâ”€â”€ types/              # TypeScript íƒ€ì… ì •ì˜
â”œâ”€â”€ utils/              # ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
â””â”€â”€ constants/          # ìƒìˆ˜ ì •ì˜
```

### 3.2 ì£¼ìš” ì»´í¬ë„ŒíŠ¸ ì„¤ê³„

#### **<ì½”ë“œ 1>** ICSR ì¼€ì´ìŠ¤ ëª©ë¡ ì»´í¬ë„ŒíŠ¸
```typescript
// src/components/cases/CaseList.tsx
import React, { useState, useCallback } from 'react';
import {
  Table, TableBody, TableCell, TableContainer, TableHead, 
  TableRow, Paper, Chip, IconButton, Tooltip
} from '@mui/material';
import { Edit, Visibility, Send } from '@mui/icons-material';
import { ICSRCase, CaseStatus } from '../../types/case.types';

interface CaseListProps {
  cases: ICSRCase[];
  loading?: boolean;
  onEdit: (caseId: string) => void;
  onView: (caseId: string) => void;
  onSubmit: (caseId: string) => void;
}

export const CaseList: React.FC<CaseListProps> = ({
  cases,
  loading = false,
  onEdit,
  onView,
  onSubmit
}) => {
  const getStatusColor = (status: CaseStatus): "success" | "warning" | "error" | "info" => {
    switch (status) {
      case 'COMPLETED': return 'success';
      case 'IN_PROGRESS': return 'warning'; 
      case 'ERROR': return 'error';
      default: return 'info';
    }
  };

  const getStatusLabel = (status: CaseStatus): string => {
    const labels = {
      DRAFT: 'ì‘ì„±ì¤‘',
      IN_PROGRESS: 'ì²˜ë¦¬ì¤‘', 
      AI_PROCESSED: 'AIì²˜ë¦¬ì™„ë£Œ',
      REVIEWED: 'ê²€í† ì™„ë£Œ',
      COMPLETED: 'ì™„ë£Œ',
      ERROR: 'ì˜¤ë¥˜'
    };
    return labels[status] || status;
  };

  return (
    <TableContainer component={Paper}>
      <Table>
        <TableHead>
          <TableRow>
            <TableCell>ì¼€ì´ìŠ¤ ë²ˆí˜¸</TableCell>
            <TableCell>í™˜ì ì´ë‹ˆì…œ</TableCell>
            <TableCell>ì˜ì•½í’ˆëª…</TableCell>
            <TableCell>ì´ìƒì‚¬ë¡€</TableCell>
            <TableCell>ìƒíƒœ</TableCell>
            <TableCell>AI ì‹ ë¢°ë„</TableCell>
            <TableCell>ìˆ˜ì •ì¼</TableCell>
            <TableCell>ì‘ì—…</TableCell>
          </TableRow>
        </TableHead>
        <TableBody>
          {cases.map((caseItem) => (
            <TableRow key={caseItem.id} hover>
              <TableCell>{caseItem.caseNumber}</TableCell>
              <TableCell>{caseItem.patient?.initials || 'N/A'}</TableCell>
              <TableCell>{caseItem.drugs?.[0]?.name || 'N/A'}</TableCell>
              <TableCell>
                {caseItem.events?.[0]?.verbatimTerm || 'N/A'}
              </TableCell>
              <TableCell>
                <Chip 
                  label={getStatusLabel(caseItem.status)}
                  color={getStatusColor(caseItem.status)}
                  size="small"
                />
              </TableCell>
              <TableCell>
                {caseItem.aiConfidence ? 
                  `${Math.round(caseItem.aiConfidence * 100)}%` : 'N/A'}
              </TableCell>
              <TableCell>
                {new Date(caseItem.updatedAt).toLocaleDateString('ko-KR')}
              </TableCell>
              <TableCell>
                <Tooltip title="ë³´ê¸°">
                  <IconButton onClick={() => onView(caseItem.id)} size="small">
                    <Visibility />
                  </IconButton>
                </Tooltip>
                <Tooltip title="í¸ì§‘">
                  <IconButton onClick={() => onEdit(caseItem.id)} size="small">
                    <Edit />
                  </IconButton>
                </Tooltip>
                {caseItem.status === 'REVIEWED' && (
                  <Tooltip title="ì œì¶œ">
                    <IconButton onClick={() => onSubmit(caseItem.id)} size="small">
                      <Send />
                    </IconButton>
                  </Tooltip>
                )}
              </TableCell>
            </TableRow>
          ))}
        </TableBody>
      </Table>
    </TableContainer>
  );
};
```

#### **<ì½”ë“œ 2>** MedDRA ìë™ ë§¤í•‘ ì»´í¬ë„ŒíŠ¸
```typescript
// src/components/meddra/MedDRAMapper.tsx
import React, { useState, useEffect } from 'react';
import {
  Autocomplete, TextField, Box, Typography, 
  Chip, LinearProgress, Alert
} from '@mui/material';
import { Psychology, CheckCircle } from '@mui/icons-material';
import { useMedDRAMappingQuery } from '../../hooks/useMedDRA';
import { MedDRAMapping, MedDRAResponse } from '../../types/meddra.types';

interface MedDRAMapperProps {
  verbatimTerm: string;
  language: 'ko' | 'en';
  onMappingSelect: (mapping: MedDRAMapping) => void;
  disabled?: boolean;
}

export const MedDRAMapper: React.FC<MedDRAMapperProps> = ({
  verbatimTerm,
  language,
  onMappingSelect,
  disabled = false
}) => {
  const [selectedMapping, setSelectedMapping] = useState<MedDRAMapping | null>(null);
  
  const {
    data: mappings,
    isLoading,
    error,
    refetch
  } = useMedDRAMappingQuery({
    verbatimTerm,
    language,
    enabled: !!verbatimTerm
  });

  const handleMappingChange = (
    event: React.SyntheticEvent,
    value: MedDRAMapping | null
  ) => {
    setSelectedMapping(value);
    if (value) {
      onMappingSelect(value);
    }
  };

  const getConfidenceColor = (confidence: number): string => {
    if (confidence >= 0.8) return '#4caf50'; // ë…¹ìƒ‰
    if (confidence >= 0.6) return '#ff9800'; // ì£¼í™©ìƒ‰
    return '#f44336'; // ë¹¨ê°„ìƒ‰
  };

  const getConfidenceLabel = (confidence: number): string => {
    if (confidence >= 0.8) return 'ë†’ìŒ';
    if (confidence >= 0.6) return 'ì¤‘ê°„';
    return 'ë‚®ìŒ';
  };

  return (
    <Box>
      <Typography variant="subtitle2" gutterBottom>
        <Psychology sx={{ mr: 1, verticalAlign: 'middle' }} />
        AI MedDRA ë§¤í•‘ ê²°ê³¼
      </Typography>
      
      {isLoading && (
        <Box>
          <LinearProgress />
          <Typography variant="caption" color="textSecondary">
            AIê°€ MedDRA ì½”ë“œë¥¼ ë¶„ì„ì¤‘ì…ë‹ˆë‹¤...
          </Typography>
        </Box>
      )}

      {error && (
        <Alert severity="error" sx={{ mb: 2 }}>
          MedDRA ë§¤í•‘ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.
        </Alert>
      )}

      {mappings && (
        <Autocomplete
          options={mappings}
          value={selectedMapping}
          onChange={handleMappingChange}
          disabled={disabled}
          getOptionLabel={(option) => 
            `${option.ptCode} - ${option.ptName} (${Math.round(option.confidence * 100)}%)`
          }
          renderInput={(params) => (
            <TextField
              {...params}
              label="MedDRA PT ì„ íƒ"
              variant="outlined"
              placeholder="AI ì¶”ì²œ ê²°ê³¼ì—ì„œ ì„ íƒí•˜ê±°ë‚˜ ì§ì ‘ ê²€ìƒ‰"
              helperText={selectedMapping ? 
                `ì„ íƒëœ ì½”ë“œ: ${selectedMapping.ptCode}` : 
                'ê°€ì¥ ë†’ì€ ì‹ ë¢°ë„ì˜ ê²°ê³¼ë¥¼ ì„ íƒí•˜ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤.'
              }
            />
          )}
          renderOption={(props, option) => (
            <Box component="li" {...props}>
              <Box sx={{ flexGrow: 1 }}>
                <Typography variant="subtitle2">
                  {option.ptCode} - {option.ptName}
                </Typography>
                <Typography variant="caption" color="textSecondary">
                  SOC: {option.socName || 'N/A'}
                </Typography>
              </Box>
              <Box sx={{ ml: 2 }}>
                <Chip
                  label={`${getConfidenceLabel(option.confidence)} ${Math.round(option.confidence * 100)}%`}
                  size="small"
                  sx={{ 
                    bgcolor: getConfidenceColor(option.confidence),
                    color: 'white'
                  }}
                />
              </Box>
            </Box>
          )}
          noOptionsText="ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤"
        />
      )}

      {selectedMapping && (
        <Box sx={{ mt: 2, p: 2, bgcolor: 'grey.100', borderRadius: 1 }}>
          <Typography variant="subtitle2" sx={{ display: 'flex', alignItems: 'center' }}>
            <CheckCircle sx={{ mr: 1, color: 'success.main' }} />
            ì„ íƒëœ MedDRA ì •ë³´
          </Typography>
          <Typography variant="body2">
            <strong>PT Code:</strong> {selectedMapping.ptCode}
          </Typography>
          <Typography variant="body2">
            <strong>PT Name:</strong> {selectedMapping.ptName}
          </Typography>
          <Typography variant="body2">
            <strong>SOC:</strong> {selectedMapping.socName || 'N/A'}
          </Typography>
          <Typography variant="body2">
            <strong>AI ì‹ ë¢°ë„:</strong> {Math.round(selectedMapping.confidence * 100)}%
          </Typography>
        </Box>
      )}
    </Box>
  );
};
```

### 3.3 ìƒíƒœ ê´€ë¦¬ (Redux)

#### **<ì½”ë“œ 3>** Case Store ì„¤ê³„
```typescript
// src/store/slices/caseSlice.ts
import { createSlice, createAsyncThunk, PayloadAction } from '@reduxjs/toolkit';
import { caseService } from '../../services/caseService';
import { ICSRCase, CaseFilters, CreateCaseRequest } from '../../types/case.types';

interface CaseState {
  cases: ICSRCase[];
  currentCase: ICSRCase | null;
  loading: boolean;
  error: string | null;
  filters: CaseFilters;
  pagination: {
    page: number;
    limit: number;
    total: number;
  };
}

const initialState: CaseState = {
  cases: [],
  currentCase: null,
  loading: false,
  error: null,
  filters: {
    status: undefined,
    dateRange: undefined,
    searchTerm: ''
  },
  pagination: {
    page: 1,
    limit: 20,
    total: 0
  }
};

// Async Thunks
export const fetchCases = createAsyncThunk(
  'cases/fetchCases',
  async (params: { filters?: CaseFilters; page?: number; limit?: number }) => {
    const response = await caseService.getCases(params);
    return response;
  }
);

export const createCase = createAsyncThunk(
  'cases/createCase',
  async (caseData: CreateCaseRequest) => {
    const response = await caseService.createCase(caseData);
    return response;
  }
);

export const processWithAI = createAsyncThunk(
  'cases/processWithAI',
  async ({ caseId, processType }: { caseId: string; processType: string }) => {
    const response = await caseService.processWithAI(caseId, processType);
    return response;
  }
);

// Slice
const caseSlice = createSlice({
  name: 'cases',
  initialState,
  reducers: {
    setFilters: (state, action: PayloadAction<Partial<CaseFilters>>) => {
      state.filters = { ...state.filters, ...action.payload };
    },
    clearError: (state) => {
      state.error = null;
    },
    setCurrentCase: (state, action: PayloadAction<ICSRCase | null>) => {
      state.currentCase = action.payload;
    },
    updateCaseInList: (state, action: PayloadAction<ICSRCase>) => {
      const index = state.cases.findIndex(c => c.id === action.payload.id);
      if (index !== -1) {
        state.cases[index] = action.payload;
      }
    }
  },
  extraReducers: (builder) => {
    builder
      // Fetch Cases
      .addCase(fetchCases.pending, (state) => {
        state.loading = true;
        state.error = null;
      })
      .addCase(fetchCases.fulfilled, (state, action) => {
        state.loading = false;
        state.cases = action.payload.cases;
        state.pagination = action.payload.pagination;
      })
      .addCase(fetchCases.rejected, (state, action) => {
        state.loading = false;
        state.error = action.error.message || 'Failed to fetch cases';
      })
      
      // Create Case
      .addCase(createCase.fulfilled, (state, action) => {
        state.cases.unshift(action.payload);
        state.pagination.total += 1;
      })
      
      // AI Processing
      .addCase(processWithAI.pending, (state, action) => {
        const caseId = action.meta.arg.caseId;
        const caseIndex = state.cases.findIndex(c => c.id === caseId);
        if (caseIndex !== -1) {
          state.cases[caseIndex].status = 'IN_PROGRESS';
        }
      })
      .addCase(processWithAI.fulfilled, (state, action) => {
        const updatedCase = action.payload;
        const caseIndex = state.cases.findIndex(c => c.id === updatedCase.id);
        if (caseIndex !== -1) {
          state.cases[caseIndex] = updatedCase;
        }
        if (state.currentCase?.id === updatedCase.id) {
          state.currentCase = updatedCase;
        }
      });
  }
});

export const { setFilters, clearError, setCurrentCase, updateCaseInList } = caseSlice.actions;
export default caseSlice.reducer;
```

---

## ğŸ”§ 4. Back-End ìƒì„¸ ì„¤ê³„

### 4.1 í”„ë¡œì íŠ¸ êµ¬ì¡°

```
src/
â”œâ”€â”€ controllers/        # API ì»¨íŠ¸ë¡¤ëŸ¬
â”œâ”€â”€ services/          # ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§
â”œâ”€â”€ models/            # ë°ì´í„° ëª¨ë¸ (Prisma)
â”œâ”€â”€ middleware/        # ë¯¸ë“¤ì›¨ì–´
â”œâ”€â”€ routes/            # ë¼ìš°íŠ¸ ì •ì˜
â”œâ”€â”€ utils/             # ìœ í‹¸ë¦¬í‹°
â”œâ”€â”€ types/             # TypeScript íƒ€ì…
â”œâ”€â”€ config/            # ì„¤ì •
â””â”€â”€ tests/             # í…ŒìŠ¤íŠ¸ íŒŒì¼
```

### 4.2 API ì„¤ê³„

#### **<í‘œ 8>** RESTful API ì—”ë“œí¬ì¸íŠ¸
| Method | Endpoint | ì„¤ëª… | ì¸ì¦ í•„ìš” |
|--------|----------|------|-----------|
| GET | /api/v1/cases | ì¼€ì´ìŠ¤ ëª©ë¡ ì¡°íšŒ | âœ… |
| POST | /api/v1/cases | ìƒˆ ì¼€ì´ìŠ¤ ìƒì„± | âœ… |
| GET | /api/v1/cases/:id | íŠ¹ì • ì¼€ì´ìŠ¤ ì¡°íšŒ | âœ… |
| PUT | /api/v1/cases/:id | ì¼€ì´ìŠ¤ ìˆ˜ì • | âœ… |
| POST | /api/v1/cases/:id/ai-process | AI ì²˜ë¦¬ ìš”ì²­ | âœ… |
| GET | /api/v1/meddra/search | MedDRA ê²€ìƒ‰ | âœ… |
| POST | /api/v1/meddra/mapping | MedDRA ìë™ ë§¤í•‘ | âœ… |
| POST | /api/v1/regulatory/submit | ê·œì œê¸°ê´€ ì œì¶œ | âœ… |

#### **<ì½”ë“œ 4>** Case Controller êµ¬í˜„
```typescript
// src/controllers/caseController.ts
import { Request, Response, NextFunction } from 'express';
import { CaseService } from '../services/caseService';
import { AIProcessingService } from '../services/aiProcessingService';
import { validationResult } from 'express-validator';
import { AppError } from '../utils/AppError';

export class CaseController {
  constructor(
    private caseService: CaseService,
    private aiService: AIProcessingService
  ) {}

  // ì¼€ì´ìŠ¤ ëª©ë¡ ì¡°íšŒ
  getCases = async (req: Request, res: Response, next: NextFunction) => {
    try {
      const { page = 1, limit = 20, status, searchTerm, dateFrom, dateTo } = req.query;
      
      const filters = {
        status: status as string,
        searchTerm: searchTerm as string,
        dateRange: dateFrom && dateTo ? {
          from: new Date(dateFrom as string),
          to: new Date(dateTo as string)
        } : undefined
      };

      const result = await this.caseService.getCases({
        page: parseInt(page as string),
        limit: parseInt(limit as string),
        filters,
        userId: req.user.id
      });

      res.json({
        success: true,
        data: result,
        message: 'Cases retrieved successfully'
      });
    } catch (error) {
      next(error);
    }
  };

  // ìƒˆ ì¼€ì´ìŠ¤ ìƒì„±
  createCase = async (req: Request, res: Response, next: NextFunction) => {
    try {
      // ì…ë ¥ ê²€ì¦
      const errors = validationResult(req);
      if (!errors.isEmpty()) {
        throw new AppError('Validation failed', 400, errors.array());
      }

      const caseData = {
        ...req.body,
        createdBy: req.user.id,
        senderID: req.user.senderID
      };

      const newCase = await this.caseService.createCase(caseData);

      res.status(201).json({
        success: true,
        data: newCase,
        message: 'Case created successfully'
      });
    } catch (error) {
      next(error);
    }
  };

  // AI ì²˜ë¦¬ ìš”ì²­
  processWithAI = async (req: Request, res: Response, next: NextFunction) => {
    try {
      const { id: caseId } = req.params;
      const { processType, options } = req.body;

      // ì¼€ì´ìŠ¤ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
      const existingCase = await this.caseService.getCaseById(caseId, req.user.id);
      if (!existingCase) {
        throw new AppError('Case not found', 404);
      }

      // AI ì²˜ë¦¬ ì‹œì‘
      const processingJob = await this.aiService.processCase(caseId, processType, options);

      res.json({
        success: true,
        data: {
          jobId: processingJob.id,
          status: 'started',
          estimatedTime: processingJob.estimatedTime
        },
        message: 'AI processing started'
      });
    } catch (error) {
      next(error);
    }
  };

  // ì¼€ì´ìŠ¤ ìƒì„¸ ì¡°íšŒ
  getCaseById = async (req: Request, res: Response, next: NextFunction) => {
    try {
      const { id } = req.params;
      const caseData = await this.caseService.getCaseById(id, req.user.id);
      
      if (!caseData) {
        throw new AppError('Case not found', 404);
      }

      res.json({
        success: true,
        data: caseData,
        message: 'Case retrieved successfully'
      });
    } catch (error) {
      next(error);
    }
  };

  // ì¼€ì´ìŠ¤ ì—…ë°ì´íŠ¸
  updateCase = async (req: Request, res: Response, next: NextFunction) => {
    try {
      const { id } = req.params;
      const updateData = req.body;

      // ì…ë ¥ ê²€ì¦
      const errors = validationResult(req);
      if (!errors.isEmpty()) {
        throw new AppError('Validation failed', 400, errors.array());
      }

      const updatedCase = await this.caseService.updateCase(id, updateData, req.user.id);

      res.json({
        success: true,
        data: updatedCase,
        message: 'Case updated successfully'
      });
    } catch (error) {
      next(error);
    }
  };
}
```

### 4.3 ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ (Service Layer)

#### **<ì½”ë“œ 5>** Case Service êµ¬í˜„
```typescript
// src/services/caseService.ts
import { PrismaClient } from '@prisma/client';
import { AIProcessingService } from './aiProcessingService';
import { XMLGeneratorService } from './xmlGeneratorService';
import { ValidationService } from './validationService';
import { AppError } from '../utils/AppError';
import { 
  CreateCaseRequest, 
  UpdateCaseRequest, 
  CaseFilters, 
  ICSRCase 
} from '../types/case.types';

export class CaseService {
  constructor(
    private prisma: PrismaClient,
    private aiService: AIProcessingService,
    private xmlService: XMLGeneratorService,
    private validationService: ValidationService
  ) {}

  async getCases(params: {
    page: number;
    limit: number;
    filters?: CaseFilters;
    userId: string;
  }) {
    const { page, limit, filters, userId } = params;
    const offset = (page - 1) * limit;

    // ê¶Œí•œì— ë”°ë¥¸ í•„í„°ë§
    const whereClause = this.buildWhereClause(filters, userId);

    const [cases, total] = await Promise.all([
      this.prisma.case.findMany({
        skip: offset,
        take: limit,
        where: whereClause,
        include: {
          patient: {
            select: {
              initials: true,
              age: true,
              gender: true
            }
          },
          drugs: {
            select: {
              name: true,
              meddraCode: true,
              dose: true,
              unit: true
            }
          },
          events: {
            select: {
              verbatimTerm: true,
              ptCode: true,
              ptName: true,
              seriousness: true
            }
          },
          aiPredictions: {
            select: {
              confidence: true,
              modelVersion: true
            },
            orderBy: {
              createdAt: 'desc'
            },
            take: 1
          }
        },
        orderBy: {
          updatedAt: 'desc'
        }
      }),
      this.prisma.case.count({ where: whereClause })
    ]);

    // AI ì‹ ë¢°ë„ ì¶”ê°€
    const casesWithConfidence = cases.map(c => ({
      ...c,
      aiConfidence: c.aiPredictions[0]?.confidence || null
    }));

    return {
      cases: casesWithConfidence,
      pagination: {
        page,
        limit,
        total,
        totalPages: Math.ceil(total / limit)
      }
    };
  }

  async createCase(data: CreateCaseRequest): Promise<ICSRCase> {
    try {
      // ì¼€ì´ìŠ¤ ë²ˆí˜¸ ìƒì„±
      const caseNumber = await this.generateCaseNumber();

      // íŠ¸ëœì­ì…˜ìœ¼ë¡œ ì¼€ì´ìŠ¤ì™€ ê´€ë ¨ ë°ì´í„° ìƒì„±
      const newCase = await this.prisma.$transaction(async (tx) => {
        // ë©”ì¸ ì¼€ì´ìŠ¤ ìƒì„±
        const caseRecord = await tx.case.create({
          data: {
            caseNumber,
            senderID: data.senderID,
            receiverID: data.receiverID || 'MFDS-O-KR',
            reportType: data.reportType || 'INITIAL',
            status: 'DRAFT',
            createdBy: data.createdBy,
            
            // í™˜ì ì •ë³´ ìƒì„±
            patient: data.patient ? {
              create: {
                initials: data.patient.initials,
                age: data.patient.age,
                gender: data.patient.gender,
                medicalHistory: data.patient.medicalHistory
              }
            } : undefined,
            
            // ì˜ì•½í’ˆ ì •ë³´ ìƒì„±
            drugs: data.drugs ? {
              create: data.drugs.map(drug => ({
                name: drug.name,
                activeIngredient: drug.activeIngredient,
                dose: drug.dose,
                unit: drug.unit,
                route: drug.route,
                indication: drug.indication
              }))
            } : undefined,
            
            // ì´ìƒì‚¬ë¡€ ì •ë³´ ìƒì„±
            events: data.events ? {
              create: data.events.map(event => ({
                verbatimTerm: event.verbatimTerm,
                onsetDate: event.onsetDate,
                seriousness: event.seriousness,
                outcome: event.outcome
              }))
            } : undefined
          },
          include: {
            patient: true,
            drugs: true,
            events: true
          }
        });

        // ê°ì‚¬ ë¡œê·¸ ìƒì„±
        await tx.auditLog.create({
          data: {
            caseId: caseRecord.id,
            userId: data.createdBy,
            action: 'CREATE_CASE',
            details: { caseNumber },
            timestamp: new Date()
          }
        });

        return caseRecord;
      });

      return newCase;
    } catch (error) {
      throw new AppError('Failed to create case', 500, error);
    }
  }

  async updateCase(caseId: string, data: UpdateCaseRequest, userId: string): Promise<ICSRCase> {
    try {
      // ê¶Œí•œ í™•ì¸
      await this.checkCasePermission(caseId, userId);

      const updatedCase = await this.prisma.$transaction(async (tx) => {
        // ì¼€ì´ìŠ¤ ì—…ë°ì´íŠ¸
        const updated = await tx.case.update({
          where: { id: caseId },
          data: {
            status: data.status,
            updatedBy: userId,
            updatedAt: new Date(),
            
            // í™˜ì ì •ë³´ ì—…ë°ì´íŠ¸
            patient: data.patient ? {
              upsert: {
                create: data.patient,
                update: data.patient
              }
            } : undefined
          },
          include: {
            patient: true,
            drugs: true,
            events: true,
            aiPredictions: {
              orderBy: { createdAt: 'desc' },
              take: 1
            }
          }
        });

        // ì˜ì•½í’ˆ ì •ë³´ ì—…ë°ì´íŠ¸
        if (data.drugs) {
          await tx.drug.deleteMany({ where: { caseId } });
          await tx.drug.createMany({
            data: data.drugs.map(drug => ({ ...drug, caseId }))
          });
        }

        // ì´ìƒì‚¬ë¡€ ì •ë³´ ì—…ë°ì´íŠ¸
        if (data.events) {
          await tx.event.deleteMany({ where: { caseId } });
          await tx.event.createMany({
            data: data.events.map(event => ({ ...event, caseId }))
          });
        }

        // ê°ì‚¬ ë¡œê·¸
        await tx.auditLog.create({
          data: {
            caseId,
            userId,
            action: 'UPDATE_CASE',
            details: data,
            timestamp: new Date()
          }
        });

        return updated;
      });

      return updatedCase;
    } catch (error) {
      throw new AppError('Failed to update case', 500, error);
    }
  }

  private buildWhereClause(filters?: CaseFilters, userId?: string) {
    const where: any = {};

    // ì‚¬ìš©ìë³„ ê¶Œí•œ í•„í„°ë§
    if (userId) {
      where.OR = [
        { createdBy: userId },
        { assignedTo: userId },
        // ê´€ë¦¬ì ê¶Œí•œ í™•ì¸ì€ middlewareì—ì„œ ì²˜ë¦¬
      ];
    }

    if (filters) {
      if (filters.status) {
        where.status = filters.status;
      }

      if (filters.searchTerm) {
        where.OR = [
          { caseNumber: { contains: filters.searchTerm } },
          { patient: { initials: { contains: filters.searchTerm } } },
          { drugs: { some: { name: { contains: filters.searchTerm } } } }
        ];
      }

      if (filters.dateRange) {
        where.createdAt = {
          gte: filters.dateRange.from,
          lte: filters.dateRange.to
        };
      }
    }

    return where;
  }

  private async generateCaseNumber(): Promise<string> {
    const date = new Date();
    const year = date.getFullYear();
    const month = String(date.getMonth() + 1).padStart(2, '0');
    
    // ì—°ì›”ë³„ ì¹´ìš´í„° ì¡°íšŒ/ì¦ê°€
    const counter = await this.prisma.caseCounter.upsert({
      where: { yearMonth: `${year}${month}` },
      update: { count: { increment: 1 } },
      create: { yearMonth: `${year}${month}`, count: 1 }
    });

    return `KR${year}${month}${String(counter.count).padStart(4, '0')}`;
  }

  private async checkCasePermission(caseId: string, userId: string): Promise<void> {
    const caseData = await this.prisma.case.findUnique({
      where: { id: caseId },
      select: { createdBy: true, assignedTo: true }
    });

    if (!caseData) {
      throw new AppError('Case not found', 404);
    }

    if (caseData.createdBy !== userId && caseData.assignedTo !== userId) {
      throw new AppError('Access denied', 403);
    }
  }
}
```

### 4.4 ë°ì´í„°ë² ì´ìŠ¤ ì„¤ê³„ (Prisma Schema)

#### **<ì½”ë“œ 6>** Prisma ìŠ¤í‚¤ë§ˆ ì •ì˜
```prisma
// prisma/schema.prisma
generator client {
  provider = "prisma-client-js"
}

datasource db {
  provider = "postgresql"
  url      = env("DATABASE_URL")
}

model User {
  id            String   @id @default(cuid())
  email         String   @unique
  username      String   @unique
  firstName     String
  lastName      String
  role          Role     @default(USER)
  senderID      String?  // MFDS Sender ID
  organization  String?
  isActive      Boolean  @default(true)
  lastLoginAt   DateTime?
  createdAt     DateTime @default(now())
  updatedAt     DateTime @updatedAt
  
  // Relations
  createdCases  Case[]   @relation("CaseCreatedBy")
  assignedCases Case[]   @relation("CaseAssignedTo")
  auditLogs     AuditLog[]
  
  @@map("users")
}

model Case {
  id              String      @id @default(cuid())
  caseNumber      String      @unique
  senderID        String
  receiverID      String
  reportType      ReportType  @default(INITIAL)
  status          CaseStatus  @default(DRAFT)
  priority        Priority    @default(NORMAL)
  
  // AI Processing
  aiProcessed     Boolean     @default(false)
  aiProcessedAt   DateTime?
  aiModelVersion  String?
  
  // Tracking
  createdBy       String
  createdUser     User        @relation("CaseCreatedBy", fields: [createdBy], references: [id])
  assignedTo      String?
  assignedUser    User?       @relation("CaseAssignedTo", fields: [assignedTo], references: [id])
  
  // Timestamps
  createdAt       DateTime    @default(now())
  updatedAt       DateTime    @updatedAt
  submittedAt     DateTime?
  
  // Relations
  patient         Patient?
  drugs           Drug[]
  events          Event[]
  aiPredictions   AIPrediction[]
  submissions     RegulatorySubmission[]
  auditLogs       AuditLog[]
  attachments     Attachment[]
  
  @@map("cases")
  @@index([senderID])
  @@index([status])
  @@index([createdAt])
}

model Patient {
  id              String    @id @default(cuid())
  caseId          String    @unique
  case            Case      @relation(fields: [caseId], references: [id], onDelete: Cascade)
  
  // Patient Information (Encrypted)
  initials        String
  dateOfBirth     DateTime?
  age             Int?
  ageUnit         AgeUnit   @default(YEARS)
  gender          Gender?
  weight          Float?
  height          Float?
  
  // Medical History
  medicalHistory  String?   // Encrypted JSON
  allergies       String?   // Encrypted JSON
  concomitantMeds String?   // Encrypted JSON
  
  createdAt       DateTime  @default(now())
  updatedAt       DateTime  @updatedAt
  
  @@map("patients")
}

model Drug {
  id                  String    @id @default(cuid())
  caseId              String
  case                Case      @relation(fields: [caseId], references: [id], onDelete: Cascade)
  
  // Drug Information
  name                String
  activeIngredient    String?
  strength            String?
  formulation         String?
  manufacturer        String?
  batchLotNumber      String?
  
  // Dosing Information
  dose                String?
  unit                String?
  frequency           String?
  route               String?
  startDate           DateTime?
  endDate             DateTime?
  
  // Indication and Action
  indication          String?
  actionTaken         String?
  
  // MedDRA Coding
  meddraCode          String?
  meddraVersion       String?
  
  // Korean Specific
  koreanProductCode   String?   // í’ˆëª©ê¸°ì¤€ì½”ë“œ
  
  createdAt           DateTime  @default(now())
  updatedAt           DateTime  @updatedAt
  
  @@map("drugs")
  @@index([caseId])
}

model Event {
  id              String      @id @default(cuid())
  caseId          String
  case            Case        @relation(fields: [caseId], references: [id], onDelete: Cascade)
  
  // Event Description
  verbatimTerm    String
  description     String?
  
  // Timing
  onsetDate       DateTime?
  endDate         DateTime?
  duration        String?
  
  // Severity and Outcome
  seriousness     Seriousness @default(NON_SERIOUS)
  severity        Severity?
  outcome         Outcome?
  
  // MedDRA Coding
  ptCode          String?
  ptName          String?
  socCode         String?
  socName         String?
  meddraVersion   String?
  
  // Causality Assessment
  causalityWHO    String?     // WHO-UMC Scale
  causalityKRCT   String?     // KRCT Scale
  
  createdAt       DateTime    @default(now())
  updatedAt       DateTime    @updatedAt
  
  @@map("events")
  @@index([caseId])
}

model AIPrediction {
  id            String    @id @default(cuid())
  caseId        String
  case          Case      @relation(fields: [caseId], references: [id], onDelete: Cascade)
  
  modelName     String
  modelVersion  String
  prediction    Json      // AI ì˜ˆì¸¡ ê²°ê³¼ (JSON)
  confidence    Float     // 0.0 - 1.0
  processingTime Float?   // ì²˜ë¦¬ ì‹œê°„ (seconds)
  
  // Human Review
  humanReviewed Boolean   @default(false)
  reviewedBy    String?
  reviewedAt    DateTime?
  accepted      Boolean?
  feedback      String?
  
  createdAt     DateTime  @default(now())
  
  @@map("ai_predictions")
  @@index([caseId])
  @@index([modelName, modelVersion])
}

model RegulatorySubmission {
  id              String            @id @default(cuid())
  caseId          String
  case            Case              @relation(fields: [caseId], references: [id], onDelete: Cascade)
  
  authority       String            // MFDS, FDA, EMA, etc.
  submissionType  SubmissionType
  xmlContent      String            // Generated XML
  
  status          SubmissionStatus  @default(PENDING)
  submittedAt     DateTime?
  acknowledgedAt  DateTime?
  ackMessage      String?           // ACK/NACK message
  
  // Retry Logic
  retryCount      Int               @default(0)
  maxRetries      Int               @default(3)
  nextRetryAt     DateTime?
  
  createdAt       DateTime          @default(now())
  updatedAt       DateTime          @updatedAt
  
  @@map("regulatory_submissions")
  @@index([caseId])
  @@index([authority])
  @@index([status])
}

model AuditLog {
  id          String    @id @default(cuid())
  caseId      String?
  case        Case?     @relation(fields: [caseId], references: [id], onDelete: Cascade)
  userId      String
  user        User      @relation(fields: [userId], references: [id])
  
  action      String    // CREATE_CASE, UPDATE_CASE, SUBMIT_CASE, etc.
  details     Json?     // Action details
  ipAddress   String?
  userAgent   String?
  
  timestamp   DateTime  @default(now())
  
  @@map("audit_logs")
  @@index([caseId])
  @@index([userId])
  @@index([timestamp])
}

model CaseCounter {
  yearMonth   String    @id  // YYYYMM format
  count       Int       @default(0)
  
  @@map("case_counters")
}

// Enums
enum Role {
  SUPER_ADMIN
  ADMIN
  MANAGER
  REVIEWER
  USER
}

enum CaseStatus {
  DRAFT
  IN_PROGRESS
  AI_PROCESSED
  REVIEWED
  APPROVED
  SUBMITTED
  COMPLETED
  ERROR
  REJECTED
}

enum ReportType {
  INITIAL
  FOLLOWUP
  CORRECTION
  NULLIFICATION
}

enum Priority {
  LOW
  NORMAL
  HIGH
  URGENT
}

enum Gender {
  MALE
  FEMALE
  UNKNOWN
}

enum AgeUnit {
  YEARS
  MONTHS
  WEEKS
  DAYS
  HOURS
}

enum Seriousness {
  SERIOUS
  NON_SERIOUS
}

enum Severity {
  MILD
  MODERATE
  SEVERE
}

enum Outcome {
  RECOVERED
  RECOVERING
  NOT_RECOVERED
  RECOVERED_WITH_SEQUELAE
  FATAL
  UNKNOWN
}

enum SubmissionType {
  INITIAL
  FOLLOWUP
  CORRECTION
  NULLIFICATION
}

enum SubmissionStatus {
  PENDING
  SUBMITTED
  ACKNOWLEDGED
  REJECTED
  ERROR
}
```

---

## ğŸ¤– 5. AI Engine ìƒì„¸ ì„¤ê³„

### 5.1 AI ì„œë¹„ìŠ¤ ì•„í‚¤í…ì²˜

#### **<ì½”ë“œ 7>** AI Processing Service
```python
# src/ai/services/ai_processing_service.py
import asyncio
import logging
from typing import Dict, List, Optional, Any
from datetime import datetime

import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from sentence_transformers import SentenceTransformer
import spacy
from sklearn.feature_extraction.text import TfidfVectorizer
import pandas as pd

from ..models.meddra_mapper import MedDRAMapper
from ..models.causality_assessor import CausalityAssessor
from ..models.seriousness_classifier import SeriousnessClassifier
from ..utils.text_preprocessor import TextPreprocessor
from ..config import AI_CONFIG

logger = logging.getLogger(__name__)

class AIProcessingService:
    """AI ê¸°ë°˜ ICSR ì²˜ë¦¬ ì„œë¹„ìŠ¤"""
    
    def __init__(self):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.models = {}
        self.tokenizers = {}
        self.text_preprocessor = TextPreprocessor()
        
        # NLP ëª¨ë¸ ë¡œë“œ
        self.nlp = spacy.load("ko_core_news_sm")  # í•œêµ­ì–´ ëª¨ë¸
        self.nlp_en = spacy.load("en_core_web_sm")  # ì˜ì–´ ëª¨ë¸
        
        # ì´ˆê¸°í™”
        self._load_models()
        
    def _load_models(self):
        """AI ëª¨ë¸ë“¤ì„ ë©”ëª¨ë¦¬ì— ë¡œë“œ"""
        try:
            logger.info("Loading AI models...")
            
            # MedDRA ë§¤í•‘ ëª¨ë¸
            self.meddra_mapper = MedDRAMapper(
                model_path=AI_CONFIG['MEDDRA_MODEL_PATH'],
                device=self.device
            )
            
            # ì¸ê³¼ê´€ê³„ í‰ê°€ ëª¨ë¸
            self.causality_assessor = CausalityAssessor(
                model_path=AI_CONFIG['CAUSALITY_MODEL_PATH'],
                device=self.device
            )
            
            # ì¤‘ëŒ€ì„± ë¶„ë¥˜ ëª¨ë¸
            self.seriousness_classifier = SeriousnessClassifier(
                model_path=AI_CONFIG['SERIOUSNESS_MODEL_PATH'],
                device=self.device
            )
            
            # BioBERT í† í¬ë‚˜ì´ì € ë° ëª¨ë¸
            self.tokenizer = AutoTokenizer.from_pretrained(
                'dmis-lab/biobert-base-cased-v1.1'
            )
            
            logger.info("All AI models loaded successfully")
            
        except Exception as e:
            logger.error(f"Error loading AI models: {str(e)}")
            raise

    async def process_case_data(
        self, 
        case_data: Dict[str, Any], 
        process_types: List[str]
    ) -> Dict[str, Any]:
        """
        ì¼€ì´ìŠ¤ ë°ì´í„°ë¥¼ AIë¡œ ì²˜ë¦¬
        
        Args:
            case_data: ICSR ì¼€ì´ìŠ¤ ë°ì´í„°
            process_types: ì²˜ë¦¬í•  AI ì‘ì—… ìœ í˜• ë¦¬ìŠ¤íŠ¸
                          ['extract', 'classify', 'meddra', 'causality', 'validate']
        
        Returns:
            ì²˜ë¦¬ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        start_time = datetime.now()
        results = {
            'case_id': case_data.get('id'),
            'processed_at': start_time,
            'processing_time': 0,
            'results': {},
            'confidence_scores': {},
            'errors': []
        }
        
        try:
            # ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ íƒœìŠ¤í¬ ìƒì„±
            tasks = []
            
            if 'extract' in process_types:
                tasks.append(self._extract_medical_entities(case_data))
                
            if 'classify' in process_types:
                tasks.append(self._classify_case_attributes(case_data))
                
            if 'meddra' in process_types:
                tasks.append(self._map_to_meddra(case_data))
                
            if 'causality' in process_types:
                tasks.append(self._assess_causality(case_data))
                
            if 'validate' in process_types:
                tasks.append(self._validate_data_quality(case_data))
            
            # ë³‘ë ¬ ì‹¤í–‰
            task_results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # ê²°ê³¼ í†µí•©
            for i, result in enumerate(task_results):
                if isinstance(result, Exception):
                    results['errors'].append(str(result))
                else:
                    results['results'].update(result)
            
            # ì „ì²´ ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚°
            confidence_scores = [
                score for score in results.get('confidence_scores', {}).values()
                if isinstance(score, (int, float))
            ]
            
            if confidence_scores:
                results['overall_confidence'] = sum(confidence_scores) / len(confidence_scores)
            else:
                results['overall_confidence'] = 0.0
            
            # ì²˜ë¦¬ ì‹œê°„ ê³„ì‚°
            end_time = datetime.now()
            results['processing_time'] = (end_time - start_time).total_seconds()
            
            logger.info(
                f"Case {case_data.get('id')} processed in "
                f"{results['processing_time']:.2f} seconds"
            )
            
        except Exception as e:
            logger.error(f"Error processing case {case_data.get('id')}: {str(e)}")
            results['errors'].append(str(e))
        
        return results

    async def _extract_medical_entities(self, case_data: Dict[str, Any]) -> Dict[str, Any]:
        """ì˜ë£Œ ê°œì²´ëª… ì¶”ì¶œ"""
        try:
            results = {
                'extracted_entities': {},
                'confidence_scores': {}
            }
            
            # ììœ í…ìŠ¤íŠ¸ í•„ë“œë“¤ì—ì„œ ê°œì²´ëª… ì¶”ì¶œ
            text_fields = [
                case_data.get('narrative', ''),
                case_data.get('medical_history', ''),
                case_data.get('event_description', '')
            ]
            
            full_text = ' '.join(filter(None, text_fields))
            
            if not full_text.strip():
                return results
            
            # ì–¸ì–´ ê°ì§€
            language = self._detect_language(full_text)
            nlp_model = self.nlp if language == 'ko' else self.nlp_en
            
            # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
            processed_text = self.text_preprocessor.preprocess(full_text, language)
            
            # spaCyë¥¼ ì‚¬ìš©í•œ ê°œì²´ëª… ì¸ì‹
            doc = nlp_model(processed_text)
            
            entities = {
                'drugs': [],
                'symptoms': [],
                'medical_conditions': [],
                'procedures': [],
                'dates': [],
                'persons': [],
                'organizations': []
            }
            
            for ent in doc.ents:
                entity_info = {
                    'text': ent.text,
                    'label': ent.label_,
                    'start': ent.start_char,
                    'end': ent.end_char,
                    'confidence': 0.8  # spaCy confidence placeholder
                }
                
                # ì—”í‹°í‹° ìœ í˜•ë³„ ë¶„ë¥˜
                if ent.label_ in ['DRUG', 'MEDICATION']:
                    entities['drugs'].append(entity_info)
                elif ent.label_ in ['SYMPTOM', 'DISEASE', 'CONDITION']:
                    entities['symptoms'].append(entity_info)
                elif ent.label_ in ['DATE', 'TIME']:
                    entities['dates'].append(entity_info)
                elif ent.label_ in ['PERSON']:
                    entities['persons'].append(entity_info)
                elif ent.label_ in ['ORG']:
                    entities['organizations'].append(entity_info)
            
            # ê·œì¹™ ê¸°ë°˜ ì¶”ê°€ ì¶”ì¶œ
            drug_entities = await self._extract_drugs_with_rules(processed_text)
            entities['drugs'].extend(drug_entities)
            
            # ì¤‘ë³µ ì œê±°
            for category in entities:
                entities[category] = self._remove_duplicate_entities(entities[category])
            
            results['extracted_entities'] = entities
            results['confidence_scores']['extraction'] = 0.85
            
            return results
            
        except Exception as e:
            logger.error(f"Error in entity extraction: {str(e)}")
            raise

    async def _map_to_meddra(self, case_data: Dict[str, Any]) -> Dict[str, Any]:
        """MedDRA ìë™ ë§¤í•‘"""
        try:
            results = {
                'meddra_mappings': [],
                'confidence_scores': {}
            }
            
            # ì´ìƒì‚¬ë¡€ ìš©ì–´ë“¤ ì¶”ì¶œ
            adverse_events = []
            
            # ì¼€ì´ìŠ¤ ë°ì´í„°ì—ì„œ ì´ìƒì‚¬ë¡€ ìš©ì–´ ìˆ˜ì§‘
            if 'events' in case_data:
                for event in case_data['events']:
                    if event.get('verbatim_term'):
                        adverse_events.append({
                            'verbatim_term': event['verbatim_term'],
                            'description': event.get('description', ''),
                            'event_id': event.get('id')
                        })
            
            if not adverse_events:
                return results
            
            # ê° ì´ìƒì‚¬ë¡€ì— ëŒ€í•´ MedDRA ë§¤í•‘ ìˆ˜í–‰
            for event in adverse_events:
                mappings = await self.meddra_mapper.map_term(
                    verbatim_term=event['verbatim_term'],
                    context=event.get('description', ''),
                    top_k=5
                )
                
                event_mapping = {
                    'event_id': event.get('event_id'),
                    'verbatim_term': event['verbatim_term'],
                    'suggested_mappings': mappings
                }
                
                results['meddra_mappings'].append(event_mapping)
            
            # ì „ì²´ ë§¤í•‘ ì‹ ë¢°ë„ ê³„ì‚°
            all_confidences = []
            for mapping in results['meddra_mappings']:
                for suggestion in mapping['suggested_mappings']:
                    all_confidences.append(suggestion['confidence'])
            
            if all_confidences:
                results['confidence_scores']['meddra_mapping'] = (
                    sum(all_confidences) / len(all_confidences)
                )
            
            return results
            
        except Exception as e:
            logger.error(f"Error in MedDRA mapping: {str(e)}")
            raise

    async def _assess_causality(self, case_data: Dict[str, Any]) -> Dict[str, Any]:
        """ì¸ê³¼ê´€ê³„ ìë™ í‰ê°€"""
        try:
            results = {
                'causality_assessments': [],
                'confidence_scores': {}
            }
            
            if not case_data.get('drugs') or not case_data.get('events'):
                return results
            
            # ì•½ë¬¼-ì´ìƒì‚¬ë¡€ ìŒë³„ ì¸ê³¼ê´€ê³„ í‰ê°€
            for drug in case_data['drugs']:
                for event in case_data['events']:
                    assessment = await self.causality_assessor.assess(
                        drug_info=drug,
                        event_info=event,
                        patient_info=case_data.get('patient', {}),
                        timing_info={
                            'drug_start': drug.get('start_date'),
                            'drug_end': drug.get('end_date'),
                            'event_onset': event.get('onset_date'),
                            'event_end': event.get('end_date')
                        }
                    )
                    
                    results['causality_assessments'].append({
                        'drug_id': drug.get('id'),
                        'drug_name': drug.get('name'),
                        'event_id': event.get('id'),
                        'event_term': event.get('verbatim_term'),
                        'who_umc_scale': assessment['who_umc'],
                        'krct_scale': assessment['krct'],
                        'confidence': assessment['confidence'],
                        'reasoning': assessment['reasoning']
                    })
            
            # í‰ê·  ì‹ ë¢°ë„ ê³„ì‚°
            confidences = [
                assessment['confidence'] 
                for assessment in results['causality_assessments']
            ]
            
            if confidences:
                results['confidence_scores']['causality'] = sum(confidences) / len(confidences)
            
            return results
            
        except Exception as e:
            logger.error(f"Error in causality assessment: {str(e)}")
            raise

    def _detect_language(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ ì–¸ì–´ ê°ì§€"""
        try:
            # í•œê¸€ ë¬¸ì ë¹„ìœ¨ ê³„ì‚°
            korean_chars = sum(1 for char in text if '\uac00' <= char <= '\ud7a3')
            total_chars = len(text.replace(' ', ''))
            
            if total_chars == 0:
                return 'en'
            
            korean_ratio = korean_chars / total_chars
            return 'ko' if korean_ratio > 0.3 else 'en'
            
        except Exception:
            return 'en'  # ê¸°ë³¸ê°’

    def _remove_duplicate_entities(self, entities: List[Dict]) -> List[Dict]:
        """ì¤‘ë³µ ê°œì²´ ì œê±°"""
        seen = set()
        unique_entities = []
        
        for entity in entities:
            key = (entity['text'].lower(), entity['label'])
            if key not in seen:
                seen.add(key)
                unique_entities.append(entity)
        
        return unique_entities

    async def _extract_drugs_with_rules(self, text: str) -> List[Dict]:
        """ê·œì¹™ ê¸°ë°˜ ì˜ì•½í’ˆ ì¶”ì¶œ"""
        # ì˜ì•½í’ˆëª… íŒ¨í„´ ë§¤ì¹­
        import re
        
        drug_patterns = [
            r'\b([A-Z][a-z]+(?:cillin|mycin|prine|zole|statin))\b',  # ì¼ë°˜ì ì¸ ì˜ì•½í’ˆ ì ‘ë¯¸ì‚¬
            r'\b(\w+(?:ì •|ìº¡ìŠ|ì‹œëŸ½|ì—°ê³ |ì£¼ì‚¬))\b',  # í•œêµ­ì–´ ì œí˜•
        ]
        
        extracted_drugs = []
        for pattern in drug_patterns:
            matches = re.finditer(pattern, text, re.IGNORECASE)
            for match in matches:
                extracted_drugs.append({
                    'text': match.group(1),
                    'label': 'DRUG',
                    'start': match.start(),
                    'end': match.end(),
                    'confidence': 0.7
                })
        
        return extracted_drugs
```

#### **<ì½”ë“œ 8>** MedDRA ë§¤í•‘ ëª¨ë¸
```python
# src/ai/models/meddra_mapper.py
import torch
import torch.nn as nn
from transformers import AutoModel, AutoTokenizer
from sentence_transformers import SentenceTransformer
import pickle
import numpy as np
from typing import List, Dict, Tuple
import logging

logger = logging.getLogger(__name__)

class MedDRAMapper:
    """MedDRA ìë™ ë§¤í•‘ì„ ìœ„í•œ AI ëª¨ë¸"""
    
    def __init__(self, model_path: str, device: torch.device):
        self.device = device
        self.model_path = model_path
        
        # ëª¨ë¸ ë° ë¦¬ì†ŒìŠ¤ ë¡œë“œ
        self._load_resources()
        
    def _load_resources(self):
        """í•„ìš”í•œ ë¦¬ì†ŒìŠ¤ë“¤ ë¡œë“œ"""
        try:
            # Sentence Transformer ëª¨ë¸ (ì˜ë¯¸ì  ìœ ì‚¬ë„)
            self.sentence_model = SentenceTransformer(
                'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'
            ).to(self.device)
            
            # MedDRA ì‚¬ì „ ë¡œë“œ
            with open(f'{self.model_path}/meddra_dict.pkl', 'rb') as f:
                self.meddra_dict = pickle.load(f)
            
            # ì‚¬ì „ ê³„ì‚°ëœ MedDRA PT ì„ë² ë”© ë¡œë“œ
            self.meddra_embeddings = np.load(f'{self.model_path}/meddra_embeddings.npy')
            
            # PT ì½”ë“œ -> ì¸ë±ìŠ¤ ë§¤í•‘
            with open(f'{self.model_path}/pt_to_idx.pkl', 'rb') as f:
                self.pt_to_idx = pickle.load(f)
            
            # ì¸ë±ìŠ¤ -> PT ì •ë³´ ë§¤í•‘
            with open(f'{self.model_path}/idx_to_pt.pkl', 'rb') as f:
                self.idx_to_pt = pickle.load(f)
            
            # í•œêµ­ì–´-ì˜ì–´ ë²ˆì—­ ë§¤í•‘ (ì„ íƒì )
            try:
                with open(f'{self.model_path}/ko_en_mapping.pkl', 'rb') as f:
                    self.ko_en_mapping = pickle.load(f)
            except FileNotFoundError:
                self.ko_en_mapping = {}
            
            logger.info("MedDRA mapping resources loaded successfully")
            
        except Exception as e:
            logger.error(f"Error loading MedDRA resources: {str(e)}")
            raise

    async def map_term(
        self, 
        verbatim_term: str, 
        context: str = "",
        top_k: int = 5,
        min_confidence: float = 0.5
    ) -> List[Dict]:
        """
        Verbatim termì„ MedDRA PTë¡œ ë§¤í•‘
        
        Args:
            verbatim_term: ì›ë³¸ ì´ìƒì‚¬ë¡€ ìš©ì–´
            context: ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸ ì •ë³´
            top_k: ë°˜í™˜í•  ìƒìœ„ ë§¤í•‘ ê°œìˆ˜
            min_confidence: ìµœì†Œ ì‹ ë¢°ë„ ì„ê³„ê°’
            
        Returns:
            ë§¤í•‘ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        try:
            # ì…ë ¥ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
            processed_term = self._preprocess_term(verbatim_term)
            
            # ì»¨í…ìŠ¤íŠ¸ê°€ ìˆìœ¼ë©´ ê²°í•©
            if context:
                search_text = f"{processed_term} {context}"
            else:
                search_text = processed_term
            
            # ì„ë² ë”© ìƒì„±
            query_embedding = self.sentence_model.encode(
                [search_text], 
                convert_to_tensor=True,
                device=self.device
            )
            
            # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
            similarities = torch.nn.functional.cosine_similarity(
                query_embedding, 
                torch.tensor(self.meddra_embeddings).to(self.device),
                dim=1
            )
            
            # ìƒìœ„ kê°œ ê²°ê³¼ ì¶”ì¶œ
            top_scores, top_indices = torch.topk(similarities, k=min(top_k * 2, len(similarities)))
            
            # ê²°ê³¼ ìƒì„±
            results = []
            for score, idx in zip(top_scores.cpu().numpy(), top_indices.cpu().numpy()):
                if score < min_confidence:
                    continue
                    
                pt_info = self.idx_to_pt[idx]
                
                # ì¶”ê°€ ìŠ¤ì½”ì–´ë§ (ê·œì¹™ ê¸°ë°˜)
                rule_score = self._calculate_rule_based_score(verbatim_term, pt_info['pt_name'])
                combined_score = (score * 0.7) + (rule_score * 0.3)
                
                result = {
                    'pt_code': pt_info['pt_code'],
                    'pt_name': pt_info['pt_name'],
                    'soc_code': pt_info.get('soc_code'),
                    'soc_name': pt_info.get('soc_name'),
                    'hlgt_code': pt_info.get('hlgt_code'),
                    'hlgt_name': pt_info.get('hlgt_name'),
                    'hlt_code': pt_info.get('hlt_code'),
                    'hlt_name': pt_info.get('hlt_name'),
                    'confidence': float(combined_score),
                    'semantic_similarity': float(score),
                    'rule_score': float(rule_score)
                }
                
                results.append(result)
            
            # ìµœì¢… ì‹ ë¢°ë„ë¡œ ì •ë ¬
            results.sort(key=lambda x: x['confidence'], reverse=True)
            
            # ìƒìœ„ kê°œë§Œ ë°˜í™˜
            return results[:top_k]
            
        except Exception as e:
            logger.error(f"Error mapping term '{verbatim_term}': {str(e)}")
            return []

    def _preprocess_term(self, term: str) -> str:
        """ìš©ì–´ ì „ì²˜ë¦¬"""
        import re
        
        # ì†Œë¬¸ì ë³€í™˜
        processed = term.lower()
        
        # íŠ¹ìˆ˜ë¬¸ì ì •ë¦¬
        processed = re.sub(r'[^\w\sê°€-í£]', ' ', processed)
        
        # ë‹¤ì¤‘ ê³µë°± ì •ë¦¬
        processed = ' '.join(processed.split())
        
        return processed.strip()

    def _calculate_rule_based_score(self, verbatim_term: str, pt_name: str) -> float:
        """ê·œì¹™ ê¸°ë°˜ ìŠ¤ì½”ì–´ ê³„ì‚°"""
        score = 0.0
        
        verbatim_lower = verbatim_term.lower()
        pt_lower = pt_name.lower()
        
        # ì •í™•í•œ ë§¤ì¹˜
        if verbatim_lower == pt_lower:
            score += 1.0
        
        # ë¶€ë¶„ ë¬¸ìì—´ ë§¤ì¹˜
        elif verbatim_lower in pt_lower or pt_lower in verbatim_lower:
            score += 0.8
        
        # ë‹¨ì–´ë³„ ë§¤ì¹˜
        verbatim_words = set(verbatim_lower.split())
        pt_words = set(pt_lower.split())
        
        if verbatim_words and pt_words:
            intersection = verbatim_words.intersection(pt_words)
            union = verbatim_words.union(pt_words)
            jaccard_score = len(intersection) / len(union)
            score += jaccard_score * 0.6
        
        # ë™ì˜ì–´ ë§¤ì¹˜ (ì‚¬ì „ ì •ì˜ëœ ë™ì˜ì–´ê°€ ìˆëŠ” ê²½ìš°)
        synonym_score = self._check_synonyms(verbatim_lower, pt_lower)
        score += synonym_score * 0.4
        
        return min(score, 1.0)

    def _check_synonyms(self, term1: str, term2: str) -> float:
        """ë™ì˜ì–´ í™•ì¸"""
        # ê°„ë‹¨í•œ ë™ì˜ì–´ ì‚¬ì „ (ì‹¤ì œë¡œëŠ” ë” í¬ê´„ì ì¸ ì‚¬ì „ í•„ìš”)
        synonyms = {
            'ë‘í†µ': ['headache', 'cephalgia'],
            'ë³µí†µ': ['abdominal pain', 'stomach ache'],
            'ì„¤ì‚¬': ['diarrhea', 'loose stool'],
            'êµ¬í† ': ['vomiting', 'emesis'],
            'ë°œì—´': ['fever', 'pyrexia'],
            'ì–´ì§€ëŸ¬ì›€': ['dizziness', 'vertigo']
        }
        
        score = 0.0
        
        for key, values in synonyms.items():
            if key in term1:
                for synonym in values:
                    if synonym in term2:
                        score = 1.0
                        break
            elif key in term2:
                for synonym in values:
                    if synonym in term1:
                        score = 1.0
                        break
        
        return score

    def retrain_model(self, training_data: List[Dict]):
        """ëª¨ë¸ ì¬í›ˆë ¨ (í”¼ë“œë°± ê¸°ë°˜)"""
        # ì‚¬ìš©ì í”¼ë“œë°±ì„ ë°”íƒ•ìœ¼ë¡œ ëª¨ë¸ ì„±ëŠ¥ ê°œì„ 
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ë” ë³µì¡í•œ ì¬í›ˆë ¨ ë¡œì§ í•„ìš”
        logger.info("Model retraining initiated with new feedback data")
        # TODO: ì¬í›ˆë ¨ ë¡œì§ êµ¬í˜„
```

### 5.2 XML ìƒì„± ì„œë¹„ìŠ¤

#### **<ì½”ë“œ 9>** E2B XML ìƒì„±ê¸°
```typescript
// src/services/xmlGeneratorService.ts
import { create } from 'xmlbuilder2';
import { ICSRCase } from '../types/case.types';
import { ValidationService } from './validationService';
import { AppError } from '../utils/AppError';

export class XMLGeneratorService {
  constructor(private validationService: ValidationService) {}

  async generateE2BXML(caseData: ICSRCase): Promise<string> {
    try {
      // ë°ì´í„° ê²€ì¦
      const validationResult = await this.validationService.validateForXML(caseData);
      if (!validationResult.isValid) {
        throw new AppError('Case data validation failed', 400, validationResult.errors);
      }

      const root = create({
        version: '1.0',
        encoding: 'UTF-8'
      }).ele('ichicsr', {
        'xmlns': 'http://www.ich.org/ICSR/xmlschema/20100718',
        'xmlns:xsi': 'http://www.w3.org/2001/XMLSchema-instance',
        'xsi:schemaLocation': 'http://www.ich.org/ICSR/xmlschema/20100718 ICH_ICSR_v2.1.xsd',
        'lang': caseData.language || 'ko'
      });

      // N.1 Message Header
      const messageHeader = root.ele('ichicsrmessageheader');
      messageHeader.ele('messagetype').txt('ichicsr');
      messageHeader.ele('messageformatversion').txt('2.1');
      messageHeader.ele('messageformatrelease').txt('2.0');
      messageHeader.ele('messagenumb').txt(caseData.batchNumber || this.generateBatchNumber());
      messageHeader.ele('messagesenderidentifier').txt(caseData.senderID);
      messageHeader.ele('messagereceiveridentifier').txt(caseData.receiverID);
      messageHeader.ele('messagedateformat').txt('102');
      messageHeader.ele('messagedate').txt(this.formatDate(new Date()));

      // N.2 Safety Report
      const safetyReport = root.ele('safetyreport');
      
      // N.2.r ICSR
      const safetyReportVersion = safetyReport.ele('safetyreportversion').txt('1');
      const safetyReportId = safetyReport.ele('safetyreportid').txt(caseData.caseNumber);
      
      // Primary Source
      const primarySource = safetyReport.ele('primarysource');
      
      if (caseData.patient) {
        // C.1 Case Information
        this.addCaseInformation(safetyReport, caseData);
        
        // C.2 Primary Source Information
        this.addPrimarySourceInformation(primarySource, caseData);
        
        // C.3 Information on Sender of the Report  
        this.addSenderInformation(safetyReport, caseData);
        
        // D Patient Information
        this.addPatientInformation(safetyReport, caseData.patient);
        
        // E Medical History
        if (caseData.patient.medicalHistory) {
          this.addMedicalHistory(safetyReport, caseData.patient);
        }
        
        // G Drug Information
        if (caseData.drugs && caseData.drugs.length > 0) {
          this.addDrugInformation(safetyReport, caseData.drugs);
        }
        
        // H Narrative Case Summary
        this.addNarrativeSummary(safetyReport, caseData);
      }

      // XML ìƒì„± ë° í¬ë§·íŒ…
      const xmlString = root.end({ 
        prettyPrint: true,
        indent: '  ',
        newline: '\n'
      });

      // ìµœì¢… ê²€ì¦
      await this.validateXMLSchema(xmlString);

      return xmlString;

    } catch (error) {
      throw new AppError('Failed to generate E2B XML', 500, error);
    }
  }

  private addCaseInformation(safetyReport: any, caseData: ICSRCase): void {
    // C.1.1 Sender's (case) safety report unique identifier
    safetyReport.ele('safetyreportid').txt(caseData.caseNumber);
    
    // C.1.2 Date of creation
    safetyReport.ele('creationdate').txt(this.formatDate(caseData.createdAt));
    
    // C.1.3 Type of report
    const reportType = this.mapReportType(caseData.reportType);
    safetyReport.ele('reporttype').txt(reportType);
    
    // C.1.4 Date of this report
    safetyReport.ele('reportdate').txt(this.formatDate(caseData.updatedAt || caseData.createdAt));
    
    // C.1.5 Date report was first received from source
    safetyReport.ele('receiptdate').txt(this.formatDate(caseData.createdAt));
    
    // C.1.6 Date of most recent information for this report
    safetyReport.ele('receivedate').txt(this.formatDate(caseData.updatedAt || caseData.createdAt));
    
    // C.1.7 Is this an expedited report?
    safetyReport.ele('serious').txt(this.isExpeditedReport(caseData) ? '1' : '2');
    
    // C.1.8.1 Worldwide unique case identification number
    safetyReport.ele('companynumb').txt(caseData.caseNumber);
  }

  private addPatientInformation(safetyReport: any, patient: any): void {
    const patientElement = safetyReport.ele('patient');
    
    // D.1 Patient (name or initials)
    if (patient.initials) {
      patientElement.ele('patientinitial').txt(patient.initials);
    }
    
    // D.2 Age information
    if (patient.age) {
      patientElement.ele('patientage').txt(patient.age.toString());
      patientElement.ele('patientageunit').txt(this.mapAgeUnit(patient.ageUnit || 'YEARS'));
    }
    
    // D.3 Date of birth
    if (patient.dateOfBirth) {
      patientElement.ele('patientdateofbirth').txt(this.formatDate(patient.dateOfBirth));
    }
    
    // D.4 Age group
    const ageGroup = this.calculateAgeGroup(patient.age);
    if (ageGroup) {
      patientElement.ele('patientagegroup').txt(ageGroup);
    }
    
    // D.5 Sex
    if (patient.gender) {
      patientElement.ele('patientsex').txt(this.mapGender(patient.gender));
    }
    
    // D.6 Last menstrual period date
    // D.7.1 Medical record number
    // D.7.2 Investigation number
    // D.8 Weight (kg)
    if (patient.weight) {
      patientElement.ele('patientweight').txt(patient.weight.toString());
    }
    
    // D.9 Height (cm)
    if (patient.height) {
      patientElement.ele('patientheight').txt(patient.height.toString());
    }
  }

  private addDrugInformation(safetyReport: any, drugs: any[]): void {
    drugs.forEach((drug, index) => {
      const drugElement = safetyReport.ele('drug');
      
      // G.k.1 Characterisation of drug role
      drugElement.ele('drugcharacterization').txt('1'); // Suspect
      
      // G.k.2.2 Medicinal product name
      if (drug.name) {
        const medProduct = drugElement.ele('medicinalproduct');
        medProduct.txt(drug.name);
      }
      
      // G.k.2.3 Substance/specified substance name
      if (drug.activeIngredient) {
        const activeSubstance = drugElement.ele('activesubstance');
        const substanceName = activeSubstance.ele('activesubstancename');
        substanceName.txt(drug.activeIngredient);
      }
      
      // G.k.3 Holder and authorisation/application number of drug
      // G.k.4 Dosage information
      if (drug.dose || drug.unit || drug.frequency) {
        const drugDosage = drugElement.ele('drugdosagetext');
        const dosageText = `${drug.dose || ''} ${drug.unit || ''} ${drug.frequency || ''}`.trim();
        drugDosage.txt(dosageText);
      }
      
      // G.k.5 Pharmaceutical dose form
      if (drug.formulation) {
        drugElement.ele('drugdosageform').txt(drug.formulation);
      }
      
      // G.k.6 Route of administration
      if (drug.route) {
        drugElement.ele('drugroute').txt(this.mapRoute(drug.route));
      }
      
      // G.k.7 Indication for use in the case
      if (drug.indication) {
        const drugIndication = drugElement.ele('drugindication');
        drugIndication.txt(drug.indication);
      }
      
      // G.k.8 Action(s) taken with drug
      if (drug.actionTaken) {
        drugElement.ele('actiondrug').txt(this.mapAction(drug.actionTaken));
      }
      
      // G.k.9 Assessment of relatedness of drug to adverse event(s)/reaction(s)
      // This would be added based on causality assessment results
      
      // Korean specific fields
      if (drug.koreanProductCode) {
        const koreanCode = drugElement.ele('drugadditionalinformation');
        // Add Korean specific MedDRA coding here
      }
    });
  }

  private addNarrativeSummary(safetyReport: any, caseData: ICSRCase): void {
    const summary = safetyReport.ele('summary');
    
    // H.1 Case narrative including clinical course, therapeutic measures, outcome
    let narrativeText = '';
    
    // í™˜ì ì •ë³´ ìš”ì•½
    if (caseData.patient) {
      narrativeText += `Patient: ${caseData.patient.initials || 'Unknown'}, `;
      narrativeText += `${caseData.patient.age || 'Unknown age'} ${caseData.patient.gender || 'Unknown gender'}. `;
    }
    
    // ì˜ì•½í’ˆ ì •ë³´ ìš”ì•½
    if (caseData.drugs && caseData.drugs.length > 0) {
      narrativeText += 'Suspected drugs: ';
      narrativeText += caseData.drugs.map(d => d.name).join(', ') + '. ';
    }
    
    // ì´ìƒì‚¬ë¡€ ì •ë³´ ìš”ì•½
    if (caseData.events && caseData.events.length > 0) {
      narrativeText += 'Adverse events: ';
      narrativeText += caseData.events.map(e => e.verbatimTerm).join(', ') + '. ';
    }
    
    // ì¶”ê°€ ì„œìˆ ì´ ìˆë‹¤ë©´ í¬í•¨
    if (caseData.narrative) {
      narrativeText += caseData.narrative;
    }
    
    summary.ele('narrativeincludeclinical').txt(narrativeText);
    
    // H.2 Other relevant history including concomitant medication and medical history
    if (caseData.patient?.medicalHistory) {
      summary.ele('reportercomment').txt(caseData.patient.medicalHistory);
    }
    
    // H.4 Sender's diagnosis/syndrome and/or reclassification of reaction/event
    // H.5 Sender's comments
  }

  // ìœ í‹¸ë¦¬í‹° ë©”ì„œë“œë“¤
  private formatDate(date: Date | string): string {
    const d = typeof date === 'string' ? new Date(date) : date;
    return d.toISOString().split('T')[0].replace(/-/g, '');
  }

  private generateBatchNumber(): string {
    const timestamp = Date.now().toString();
    return `BATCH_${timestamp}`;
  }

  private mapReportType(reportType: string): string {
    const mapping: { [key: string]: string } = {
      'INITIAL': '1',
      'FOLLOWUP': '2',
      'CORRECTION': '3',
      'NULLIFICATION': '4'
    };
    return mapping[reportType] || '1';
  }

  private mapGender(gender: string): string {
    const mapping: { [key: string]: string } = {
      'MALE': '1',
      'FEMALE': '2',
      'UNKNOWN': '0'
    };
    return mapping[gender] || '0';
  }

  private mapAgeUnit(unit: string): string {
    const mapping: { [key: string]: string } = {
      'YEARS': '801',
      'MONTHS': '802', 
      'WEEKS': '803',
      'DAYS': '804',
      'HOURS': '805'
    };
    return mapping[unit] || '801';
  }

  private calculateAgeGroup(age?: number): string | null {
    if (!age) return null;
    
    if (age < 18) return '1'; // Pediatric
    if (age >= 18 && age < 65) return '2'; // Adult
    if (age >= 65) return '3'; // Elderly
    
    return null;
  }

  private isExpeditedReport(caseData: ICSRCase): boolean {
    // ì¤‘ëŒ€í•œ ì´ìƒì‚¬ë¡€ì¸ì§€ í™•ì¸
    return caseData.events?.some(event => 
      event.seriousness === 'SERIOUS'
    ) || false;
  }

  private mapRoute(route: string): string {
    const mapping: { [key: string]: string } = {
      'ORAL': '001',
      'INTRAVENOUS': '002',
      'INTRAMUSCULAR': '003',
      'SUBCUTANEOUS': '004',
      'TOPICAL': '005'
    };
    return mapping[route.toUpperCase()] || '001';
  }

  private mapAction(action: string): string {
    const mapping: { [key: string]: string } = {
      'DOSE_REDUCED': '1',
      'DOSE_INCREASED': '2', 
      'DOSE_NOT_CHANGED': '3',
      'UNKNOWN': '4',
      'NOT_APPLICABLE': '5',
      'DRUG_WITHDRAWN': '6'
    };
    return mapping[action.toUpperCase()] || '4';
  }

  private async validateXMLSchema(xmlString: string): Promise<void> {
    // XML ìŠ¤í‚¤ë§ˆ ê²€ì¦ ë¡œì§
    // ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ICH E2B XSD ìŠ¤í‚¤ë§ˆë¥¼ ì‚¬ìš©í•˜ì—¬ ê²€ì¦
    try {
      // libxmljs2 ë˜ëŠ” ë‹¤ë¥¸ XML ê²€ì¦ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©
      // const result = validator.validate(xmlString, schemaPath);
      // if (!result.isValid) {
      //   throw new Error('XML schema validation failed');
      // }
    } catch (error) {
      throw new AppError('XML schema validation failed', 400, error);
    }
  }
}
```

---

## ğŸ§ª 6. í…ŒìŠ¤íŠ¸ ì „ëµ ë° êµ¬í˜„

### 6.1 í…ŒìŠ¤íŠ¸ ê³„íš

#### **<í‘œ 9>** í…ŒìŠ¤íŠ¸ ë§¤íŠ¸ë¦­ìŠ¤
| í…ŒìŠ¤íŠ¸ ë ˆë²¨ | ë„êµ¬/í”„ë ˆì„ì›Œí¬ | ì»¤ë²„ë¦¬ì§€ ëª©í‘œ | ìë™í™” | ì‹¤í–‰ ì£¼ê¸° |
|------------|----------------|-------------|--------|-----------|
| ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ | Jest, PyTest | 90% | âœ… | ë§¤ ì»¤ë°‹ |
| í†µí•© í…ŒìŠ¤íŠ¸ | Supertest, TestContainers | 80% | âœ… | PR ìƒì„±ì‹œ |
| API í…ŒìŠ¤íŠ¸ | Postman/Newman | 100% | âœ… | ë°°í¬ ì „ |
| E2E í…ŒìŠ¤íŠ¸ | Playwright | í•µì‹¬ í”Œë¡œìš° | âœ… | ë§¤ì¼ |
| ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ | K6, Artillery | N/A | âœ… | ì£¼ê°„ |
| ë³´ì•ˆ í…ŒìŠ¤íŠ¸ | OWASP ZAP | N/A | âœ… | ì›”ê°„ |
| AI ëª¨ë¸ í…ŒìŠ¤íŠ¸ | Custom | ì •í™•ë„ 90%+ | âœ… | ëª¨ë¸ ì—…ë°ì´íŠ¸ì‹œ |

### 6.2 í…ŒìŠ¤íŠ¸ êµ¬í˜„

#### **<ì½”ë“œ 10>** ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ì˜ˆì œ
```typescript
// src/services/__tests__/caseService.test.ts
import { CaseService } from '../caseService';
import { PrismaClient } from '@prisma/client';
import { mockDeep, mockReset } from 'jest-mock-extended';

const mockPrisma = mockDeep<PrismaClient>();
const mockAIService = {
  processCase: jest.fn(),
  mapToMedDRA: jest.fn()
};
const mockXMLService = {
  generateE2BXML: jest.fn()
};
const mockValidationService = {
  validateCase: jest.fn()
};

describe('CaseService', () => {
  let caseService: CaseService;

  beforeEach(() => {
    mockReset(mockPrisma);
    jest.clearAllMocks();
    
    caseService = new CaseService(
      mockPrisma,
      mockAIService as any,
      mockXMLService as any,
      mockValidationService as any
    );
  });

  describe('createCase', () => {
    it('should create a new ICSR case successfully', async () => {
      // Arrange
      const caseData = {
        senderID: 'MFDS-TEST-001',
        receiverID: 'MFDS-O-KR',
        reportType: 'INITIAL' as const,
        createdBy: 'user-123',
        patient: {
          initials: 'J.D.',
          age: 45,
          gender: 'MALE' as const
        },
        drugs: [{
          name: 'Aspirin',
          activeIngredient: 'Acetylsalicylic acid',
          dose: '100',
          unit: 'mg'
        }],
        events: [{
          verbatimTerm: 'Headache',
          seriousness: 'NON_SERIOUS' as const
        }]
      };

      const expectedCase = {
        id: 'case-123',
        caseNumber: 'KR202509120001',
        ...caseData,
        status: 'DRAFT',
        createdAt: new Date(),
        updatedAt: new Date()
      };

      mockPrisma.$transaction.mockImplementation(async (callback) => {
        const mockTx = {
          case: {
            create: jest.fn().mockResolvedValue(expectedCase)
          },
          auditLog: {
            create: jest.fn().mockResolvedValue({})
          }
        };
        return await callback(mockTx as any);
      });

      // Act
      const result = await caseService.createCase(caseData);

      // Assert
      expect(result).toEqual(expectedCase);
      expect(mockPrisma.$transaction).toHaveBeenCalledTimes(1);
    });

    it('should handle database errors gracefully', async () => {
      // Arrange
      const caseData = {
        senderID: 'MFDS-TEST-001',
        createdBy: 'user-123'
      };

      mockPrisma.$transaction.mockRejectedValue(new Error('Database error'));

      // Act & Assert
      await expect(caseService.createCase(caseData))
        .rejects
        .toThrow('Failed to create case');
    });
  });

  describe('getCases', () => {
    it('should return paginated cases with filters', async () => {
      // Arrange
      const params = {
        page: 1,
        limit: 10,
        filters: { status: 'DRAFT' as const },
        userId: 'user-123'
      };

      const mockCases = [
        {
          id: 'case-1',
          caseNumber: 'KR202509120001',
          status: 'DRAFT',
          aiPredictions: [{ confidence: 0.85 }],
          patient: { initials: 'J.D.' },
          drugs: [{ name: 'Aspirin' }],
          events: [{ verbatimTerm: 'Headache' }]
        }
      ];

      mockPrisma.case.findMany.mockResolvedValue(mockCases);
      mockPrisma.case.count.mockResolvedValue(1);

      // Act
      const result = await caseService.getCases(params);

      // Assert
      expect(result.cases).toHaveLength(1);
      expect(result.pagination).toEqual({
        page: 1,
        limit: 10,
        total: 1,
        totalPages: 1
      });
      expect(result.cases[0].aiConfidence).toBe(0.85);
    });
  });
});
```

#### **<ì½”ë“œ 11>** AI ëª¨ë¸ í…ŒìŠ¤íŠ¸
```python
# tests/ai/test_meddra_mapper.py
import pytest
import torch
from unittest.mock import Mock, patch
from src.ai.models.meddra_mapper import MedDRAMapper

class TestMedDRAMapper:
    @pytest.fixture
    def mock_mapper(self):
        with patch('src.ai.models.meddra_mapper.SentenceTransformer'), \
             patch('builtins.open'), \
             patch('pickle.load'), \
             patch('numpy.load'):
            
            mapper = MedDRAMapper('/fake/path', torch.device('cpu'))
            
            # Mock resources
            mapper.meddra_embeddings = torch.randn(1000, 384).numpy()
            mapper.idx_to_pt = {
                0: {
                    'pt_code': '10019211',
                    'pt_name': 'Headache',
                    'soc_code': '10029205', 
                    'soc_name': 'Nervous system disorders'
                },
                1: {
                    'pt_code': '10000081',
                    'pt_name': 'Abdominal pain',
                    'soc_code': '10017947',
                    'soc_name': 'Gastrointestinal disorders'
                }
            }
            
            return mapper

    @pytest.mark.asyncio
    async def test_map_term_korean_input(self, mock_mapper):
        """í•œêµ­ì–´ ì…ë ¥ì— ëŒ€í•œ MedDRA ë§¤í•‘ í…ŒìŠ¤íŠ¸"""
        # Arrange
        mock_mapper.sentence_model.encode.return_value = torch.randn(1, 384)
        
        # Act
        results = await mock_mapper.map_term("ì‹¬í•œ ë‘í†µ", context="", top_k=3)
        
        # Assert
        assert len(results) <= 3
        assert all(0 <= result['confidence'] <= 1 for result in results)
        assert all('pt_code' in result for result in results)
        assert all('pt_name' in result for result in results)

    @pytest.mark.asyncio
    async def test_map_term_with_context(self, mock_mapper):
        """ì»¨í…ìŠ¤íŠ¸ë¥¼ í¬í•¨í•œ ë§¤í•‘ í…ŒìŠ¤íŠ¸"""
        # Arrange
        mock_mapper.sentence_model.encode.return_value = torch.randn(1, 384)
        
        # Act
        results = await mock_mapper.map_term(
            "headache", 
            context="patient experienced severe headache after taking medication",
            top_k=5
        )
        
        # Assert
        assert len(results) <= 5
        assert results[0]['confidence'] >= 0.5  # ìµœì†Œ ì‹ ë¢°ë„ í™•ì¸

    def test_preprocess_term(self, mock_mapper):
        """ìš©ì–´ ì „ì²˜ë¦¬ í…ŒìŠ¤íŠ¸"""
        # Test cases
        test_cases = [
            ("SEVERE HEADACHE!!!", "severe headache"),
            ("ë³µí†µ   ë°   ì„¤ì‚¬", "ë³µí†µ ë° ì„¤ì‚¬"),
            ("Head-ache (severe)", "head ache severe")
        ]
        
        for input_term, expected in test_cases:
            result = mock_mapper._preprocess_term(input_term)
            assert result == expected

    def test_calculate_rule_based_score(self, mock_mapper):
        """ê·œì¹™ ê¸°ë°˜ ìŠ¤ì½”ì–´ ê³„ì‚° í…ŒìŠ¤íŠ¸"""
        # Exact match
        score = mock_mapper._calculate_rule_based_score("headache", "Headache")
        assert score >= 0.8
        
        # Partial match
        score = mock_mapper._calculate_rule_based_score("severe headache", "Headache")
        assert 0.5 <= score <= 0.8
        
        # No match
        score = mock_mapper._calculate_rule_based_score("fever", "Headache")
        assert score < 0.3

    @pytest.mark.asyncio
    async def test_map_term_empty_input(self, mock_mapper):
        """ë¹ˆ ì…ë ¥ì— ëŒ€í•œ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸"""
        results = await mock_mapper.map_term("", context="")
        assert results == []

    @pytest.mark.asyncio
    async def test_map_term_low_confidence_filtering(self, mock_mapper):
        """ë‚®ì€ ì‹ ë¢°ë„ ê²°ê³¼ í•„í„°ë§ í…ŒìŠ¤íŠ¸"""
        # Mock low confidence scores
        mock_mapper.sentence_model.encode.return_value = torch.randn(1, 384)
        
        results = await mock_mapper.map_term(
            "unknown symptom", 
            top_k=10,
            min_confidence=0.8
        )
        
        # ë†’ì€ ì„ê³„ê°’ìœ¼ë¡œ ì¸í•´ ê²°ê³¼ê°€ ì ê±°ë‚˜ ì—†ì–´ì•¼ í•¨
        assert len(results) <= 10
        assert all(result['confidence'] >= 0.8 for result in results)
```

#### **<ì½”ë“œ 12>** E2E í…ŒìŠ¤íŠ¸
```typescript
// tests/e2e/case-management.spec.ts
import { test, expect } from '@playwright/test';

test.describe('ICSR Case Management', () => {
  test.beforeEach(async ({ page }) => {
    // ë¡œê·¸ì¸
    await page.goto('/login');
    await page.fill('input[name="email"]', 'test@example.com');
    await page.fill('input[name="password"]', 'password123');
    await page.click('button[type="submit"]');
    
    // ëŒ€ì‹œë³´ë“œë¡œ ì´ë™ í™•ì¸
    await expect(page).toHaveURL('/dashboard');
  });

  test('should create new ICSR case with AI processing', async ({ page }) => {
    // ìƒˆ ì¼€ì´ìŠ¤ ìƒì„± í˜ì´ì§€ë¡œ ì´ë™
    await page.click('text=ìƒˆ ì¼€ì´ìŠ¤ ìƒì„±');
    await expect(page).toHaveURL('/cases/new');

    // í™˜ì ì •ë³´ ì…ë ¥
    await page.fill('input[name="patient.initials"]', 'J.D.');
    await page.fill('input[name="patient.age"]', '45');
    await page.selectOption('select[name="patient.gender"]', 'MALE');

    // ì˜ì•½í’ˆ ì •ë³´ ì…ë ¥
    await page.click('button:has-text("ì˜ì•½í’ˆ ì¶”ê°€")');
    await page.fill('input[name="drugs.0.name"]', 'Aspirin 100mg');
    await page.fill('input[name="drugs.0.dose"]', '100');
    await page.selectOption('select[name="drugs.0.unit"]', 'mg');

    // ì´ìƒì‚¬ë¡€ ì •ë³´ ì…ë ¥
    await page.click('button:has-text("ì´ìƒì‚¬ë¡€ ì¶”ê°€")');
    await page.fill('input[name="events.0.verbatimTerm"]', 'ì‹¬í•œ ë‘í†µ');
    await page.fill('textarea[name="events.0.description"]', 'ì•½ë¬¼ ë³µìš© í›„ 30ë¶„ ë’¤ ë°œìƒí•œ ì‹¬í•œ ë‘í†µ');

    // ì¼€ì´ìŠ¤ ì €ì¥
    await page.click('button:has-text("ì €ì¥")');

    // ì„±ê³µ ë©”ì‹œì§€ í™•ì¸
    await expect(page.locator('.alert-success')).toContainText('ì¼€ì´ìŠ¤ê°€ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤');

    // AI ì²˜ë¦¬ ì‹œì‘
    await page.click('button:has-text("AI ì²˜ë¦¬ ì‹œì‘")');
    
    // AI ì²˜ë¦¬ ì§„í–‰ í™•ì¸
    await expect(page.locator('.ai-processing-indicator')).toBeVisible();
    await expect(page.locator('text=AIê°€ ë°ì´í„°ë¥¼ ë¶„ì„ì¤‘ì…ë‹ˆë‹¤')).toBeVisible();

    // AI ì²˜ë¦¬ ì™„ë£Œ ëŒ€ê¸° (ìµœëŒ€ 30ì´ˆ)
    await expect(page.locator('.ai-processing-complete')).toBeVisible({ timeout: 30000 });

    // MedDRA ë§¤í•‘ ê²°ê³¼ í™•ì¸
    const meddraSuggestion = page.locator('.meddra-suggestion').first();
    await expect(meddraSuggestion).toContainText('Headache');
    await expect(meddraSuggestion).toContainText('10019211'); // PT Code

    // AI ì œì•ˆ ìŠ¹ì¸
    await page.click('.meddra-suggestion .accept-button');

    // ì¼€ì´ìŠ¤ ìƒíƒœ í™•ì¸
    await expect(page.locator('.case-status')).toContainText('AIì²˜ë¦¬ì™„ë£Œ');
  });

  test('should validate and submit case to regulatory authority', async ({ page }) => {
    // ê¸°ì¡´ ì¼€ì´ìŠ¤ë¡œ ì´ë™ (AI ì²˜ë¦¬ ì™„ë£Œ ìƒíƒœ)
    await page.goto('/cases/test-case-123');

    // ì¼€ì´ìŠ¤ ê²€í†  ë° ìŠ¹ì¸
    await page.click('button:has-text("ê²€í†  ì™„ë£Œ")');
    await page.fill('textarea[name="reviewComments"]', 'ëª¨ë“  í•­ëª©ì´ ì ì ˆíˆ ê¸°ì¬ë˜ì—ˆìŒ');
    await page.click('button:has-text("ìŠ¹ì¸")');

    // ê·œì œê¸°ê´€ ì œì¶œ
    await page.click('button:has-text("ê·œì œê¸°ê´€ ì œì¶œ")');
    
    // ì œì¶œ í™•ì¸ ëª¨ë‹¬
    await expect(page.locator('.submit-confirmation-modal')).toBeVisible();
    await page.selectOption('select[name="targetAuthority"]', 'MFDS');
    await page.click('button:has-text("ì œì¶œ í™•ì¸")');

    // ì œì¶œ ì§„í–‰ ìƒíƒœ í™•ì¸
    await expect(page.locator('.submission-progress')).toBeVisible();
    await expect(page.locator('text=XML ìƒì„±ì¤‘')).toBeVisible();

    // ì œì¶œ ì™„ë£Œ í™•ì¸ (ìµœëŒ€ 60ì´ˆ)
    await expect(page.locator('.submission-success')).toBeVisible({ timeout: 60000 });
    await expect(page.locator('text=MFDSì— ì„±ê³µì ìœ¼ë¡œ ì œì¶œë˜ì—ˆìŠµë‹ˆë‹¤')).toBeVisible();

    // ACK ë©”ì‹œì§€ í™•ì¸
    await page.click('button:has-text("ì œì¶œ ìƒì„¸ ì •ë³´")');
    await expect(page.locator('.ack-message')).toContainText('ìŠ¹ì¸ë¨ (AA)');
  });

  test('should handle AI processing errors gracefully', async ({ page }) => {
    // AI ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí•˜ëŠ” ì‹œë‚˜ë¦¬ì˜¤
    await page.goto('/cases/error-case-456');

    // AI ì²˜ë¦¬ ì‹œì‘
    await page.click('button:has-text("AI ì²˜ë¦¬ ì¬ì‹œë„")');

    // ì˜¤ë¥˜ ë°œìƒ ì‹œ ì²˜ë¦¬
    await expect(page.locator('.error-alert')).toBeVisible({ timeout: 10000 });
    await expect(page.locator('.error-alert')).toContainText('AI ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤');

    // ìˆ˜ë™ ì²˜ë¦¬ë¡œ ì „í™˜ ì˜µì…˜ í™•ì¸
    await expect(page.locator('button:has-text("ìˆ˜ë™ìœ¼ë¡œ ì²˜ë¦¬")')).toBeVisible();
    
    // ì˜¤ë¥˜ ì„¸ë¶€ ì •ë³´ í™•ì¸
    await page.click('button:has-text("ì˜¤ë¥˜ ì„¸ë¶€ì •ë³´")');
    await expect(page.locator('.error-details')).toContainText('ëª¨ë¸ ì„œë²„ ì—°ê²° ì‹¤íŒ¨');
  });

  test('should display real-time dashboard metrics', async ({ page }) => {
    await page.goto('/dashboard');

    // ì£¼ìš” ë©”íŠ¸ë¦­ í™•ì¸
    await expect(page.locator('.metric-total-cases')).toContainText(/\d+/);
    await expect(page.locator('.metric-ai-processed')).toContainText(/\d+/);
    await expect(page.locator('.metric-accuracy')).toContainText(/\d+%/);

    // ì°¨íŠ¸ ë¡œë”© í™•ì¸
    await expect(page.locator('.processing-time-chart')).toBeVisible();
    await expect(page.locator('.ai-accuracy-chart')).toBeVisible();

    // ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸ í™•ì¸ (5ì´ˆ ëŒ€ê¸°)
    const initialCaseCount = await page.locator('.metric-total-cases').textContent();
    await page.waitForTimeout(5000);
    
    // ìƒˆë¡œê³ ì¹¨ ì—†ì´ ë°ì´í„° ì—…ë°ì´íŠ¸ í™•ì¸
    await expect(page.locator('.last-updated')).toContainText('ë°©ê¸ˆ ì „');
  });
});
```

---

## ğŸš€ 7. ë°°í¬ ë° DevOps

### 7.1 CI/CD íŒŒì´í”„ë¼ì¸

#### **<ì½”ë“œ 13>** GitHub Actions ì›Œí¬í”Œë¡œìš°
```yaml
# .github/workflows/deploy.yml
name: CI/CD Pipeline

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]

env:
  NODE_VERSION: '20.x'
  PYTHON_VERSION: '3.11'
  DOCKER_REGISTRY: 'your-registry.com'

jobs:
  test:
    runs-on: ubuntu-latest
    
    services:
      postgres:
        image: postgres:16
        env:
          POSTGRES_PASSWORD: testpassword
          POSTGRES_DB: icsr_test
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
      
      redis:
        image: redis:7
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          npm ci
          pip install -r requirements.txt
          pip install -r requirements-test.txt

      - name: Run linting
        run: |
          npm run lint
          flake8 src/
          black --check src/

      - name: Run unit tests
        env:
          DATABASE_URL: postgresql://postgres:testpassword@localhost:5432/icsr_test
          REDIS_URL: redis://localhost:6379
        run: |
          npm run test:coverage
          pytest tests/ --cov=src --cov-report=xml

      - name: Run integration tests
        env:
          DATABASE_URL: postgresql://postgres:testpassword@localhost:5432/icsr_test
          REDIS_URL: redis://localhost:6379
        run: |
          npm run test:integration
          python -m pytest tests/integration/

      - name: Upload coverage reports
        uses: codecov/codecov-action@v3
        with:
          files: ./coverage/lcov.info,./coverage.xml
          flags: unittests
          name: icsr-ai-coverage

      - name: Run security scan
        run: |
          npm audit --audit-level=high
          safety check -r requirements.txt

  build:
    needs: test
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'

    outputs:
      frontend-image: ${{ steps.build-frontend.outputs.image }}
      backend-image: ${{ steps.build-backend.outputs.image }}
      ai-engine-image: ${{ steps.build-ai.outputs.image }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Login to Registry
        uses: docker/login-action@v3
        with:
          registry: ${{ env.DOCKER_REGISTRY }}
          username: ${{ secrets.REGISTRY_USERNAME }}
          password: ${{ secrets.REGISTRY_PASSWORD }}

      - name: Build and push Frontend
        id: build-frontend
        uses: docker/build-push-action@v5
        with:
          context: ./frontend
          file: ./frontend/Dockerfile
          push: true
          tags: |
            ${{ env.DOCKER_REGISTRY }}/icsr-ai-frontend:latest
            ${{ env.DOCKER_REGISTRY }}/icsr-ai-frontend:${{ github.sha }}
          cache-from: type=gha
          cache-to: type=gha,mode=max

      - name: Build and push Backend
        id: build-backend
        uses: docker/build-push-action@v5
        with:
          context: ./backend
          file: ./backend/Dockerfile
          push: true
          tags: |
            ${{ env.DOCKER_REGISTRY }}/icsr-ai-backend:latest
            ${{ env.DOCKER_REGISTRY }}/icsr-ai-backend:${{ github.sha }}
          cache-from: type=gha
          cache-to: type=gha,mode=max

      - name: Build and push AI Engine
        id: build-ai
        uses: docker/build-push-action@v5
        with:
          context: ./ai-engine
          file: ./ai-engine/Dockerfile
          push: true
          tags: |
            ${{ env.DOCKER_REGISTRY }}/icsr-ai-engine:latest
            ${{ env.DOCKER_REGISTRY }}/icsr-ai-engine:${{ github.sha }}
          cache-from: type=gha
          cache-to: type=gha,mode=max

  deploy-staging:
    needs: build
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/develop'
    environment: staging

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Deploy to Staging
        run: |
          # Kubernetes ë°°í¬ ìŠ¤í¬ë¦½íŠ¸
          envsubst < k8s/staging/deployment.yaml | kubectl apply -f -
          kubectl rollout status deployment/icsr-ai-backend -n staging
          kubectl rollout status deployment/icsr-ai-frontend -n staging
          kubectl rollout status deployment/icsr-ai-engine -n staging

      - name: Run E2E tests
        run: |
          npx playwright test --config=playwright.staging.config.ts

      - name: Performance testing
        run: |
          k6 run tests/performance/load-test.js

  deploy-production:
    needs: build
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    environment: production

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Blue-Green Deployment
        run: |
          # ë¸”ë£¨-ê·¸ë¦° ë°°í¬ ìŠ¤í¬ë¦½íŠ¸
          ./scripts/blue-green-deploy.sh production ${{ github.sha }}

      - name: Health Check
        run: |
          # í—¬ìŠ¤ ì²´í¬ ìŠ¤í¬ë¦½íŠ¸
          ./scripts/health-check.sh https://icsr-ai.prod.com/health

      - name: Rollback on Failure
        if: failure()
        run: |
          ./scripts/rollback.sh production

  notify:
    needs: [test, build, deploy-production]
    runs-on: ubuntu-latest
    if: always()

    steps:
      - name: Slack Notification
        uses: 8398a7/action-slack@v3
        with:
          status: ${{ job.status }}
          channel: '#icsr-ai-deployment'
          webhook_url: ${{ secrets.SLACK_WEBHOOK }}
          fields: repo,message,commit,author,action,eventName,ref,workflow
```

### 7.2 Kubernetes ë°°í¬ êµ¬ì„±

#### **<ì½”ë“œ 14>** Kubernetes ë°°í¬ ë§¤ë‹ˆí˜ìŠ¤íŠ¸
```yaml
# k8s/production/namespace.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: icsr-ai-prod
  labels:
    name: icsr-ai-prod
    environment: production

---
# k8s/production/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: icsr-ai-config
  namespace: icsr-ai-prod
data:
  NODE_ENV: "production"
  LOG_LEVEL: "info"
  DATABASE_HOST: "postgres-cluster.icsr-ai-prod.svc.cluster.local"
  REDIS_HOST: "redis-cluster.icsr-ai-prod.svc.cluster.local"
  AI_MODEL_ENDPOINT: "http://icsr-ai-engine.icsr-ai-prod.svc.cluster.local:8000"
  MFDS_ENDPOINT: "https://nedrug.mfds.go.kr/api"

---
# k8s/production/secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: icsr-ai-secrets
  namespace: icsr-ai-prod
type: Opaque
data:
  DATABASE_PASSWORD: <base64-encoded-password>
  JWT_SECRET: <base64-encoded-secret>
  ENCRYPTION_KEY: <base64-encoded-key>
  MFDS_API_KEY: <base64-encoded-api-key>

---
# k8s/production/backend-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: icsr-ai-backend
  namespace: icsr-ai-prod
  labels:
    app: icsr-ai-backend
    version: v1
spec:
  replicas: 3
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  selector:
    matchLabels:
      app: icsr-ai-backend
  template:
    metadata:
      labels:
        app: icsr-ai-backend
        version: v1
    spec:
      containers:
      - name: backend
        image: your-registry.com/icsr-ai-backend:latest
        ports:
        - containerPort: 3000
          name: http
        env:
        - name: PORT
          value: "3000"
        envFrom:
        - configMapRef:
            name: icsr-ai-config
        - secretRef:
            name: icsr-ai-secrets
        
        resources:
          requests:
            memory: "512Mi"
            cpu: "250m"
          limits:
            memory: "1Gi"
            cpu: "500m"
        
        livenessProbe:
          httpGet:
            path: /health
            port: 3000
          initialDelaySeconds: 30
          periodSeconds: 30
          timeoutSeconds: 10
          failureThreshold: 3
        
        readinessProbe:
          httpGet:
            path: /ready
            port: 3000
          initialDelaySeconds: 5
          periodSeconds: 5
          timeoutSeconds: 5
          failureThreshold: 3

      - name: sidecar-proxy
        image: nginx:1.24
        ports:
        - containerPort: 8080
          name: proxy
        volumeMounts:
        - name: nginx-config
          mountPath: /etc/nginx/conf.d
        resources:
          requests:
            memory: "64Mi"
            cpu: "50m"
          limits:
            memory: "128Mi"
            cpu: "100m"

      volumes:
      - name: nginx-config
        configMap:
          name: nginx-sidecar-config

      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - icsr-ai-backend
              topologyKey: kubernetes.io/hostname

---
# k8s/production/ai-engine-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: icsr-ai-engine
  namespace: icsr-ai-prod
  labels:
    app: icsr-ai-engine
spec:
  replicas: 2
  selector:
    matchLabels:
      app: icsr-ai-engine
  template:
    metadata:
      labels:
        app: icsr-ai-engine
    spec:
      containers:
      - name: ai-engine
        image: your-registry.com/icsr-ai-engine:latest
        ports:
        - containerPort: 8000
          name: http
        env:
        - name: CUDA_VISIBLE_DEVICES
          value: "0"
        - name: MODEL_PATH
          value: "/models"
        
        resources:
          requests:
            memory: "4Gi"
            cpu: "1000m"
            nvidia.com/gpu: 1
          limits:
            memory: "8Gi"
            cpu: "2000m"
            nvidia.com/gpu: 1
        
        volumeMounts:
        - name: model-storage
          mountPath: /models
          readOnly: true
        - name: tmp-storage
          mountPath: /tmp
        
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 60
          periodSeconds: 30
          timeoutSeconds: 20
        
        readinessProbe:
          httpGet:
            path: /ready
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 10

      volumes:
      - name: model-storage
        persistentVolumeClaim:
          claimName: ai-model-pvc
      - name: tmp-storage
        emptyDir:
          sizeLimit: 10Gi

      nodeSelector:
        accelerator: nvidia-tesla-v100
      
      tolerations:
      - key: nvidia.com/gpu
        operator: Exists
        effect: NoSchedule

---
# k8s/production/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: icsr-ai-backend
  namespace: icsr-ai-prod
  labels:
    app: icsr-ai-backend
spec:
  selector:
    app: icsr-ai-backend
  ports:
  - name: http
    port: 80
    targetPort: 3000
    protocol: TCP
  type: ClusterIP

---
apiVersion: v1
kind: Service
metadata:
  name: icsr-ai-engine
  namespace: icsr-ai-prod
  labels:
    app: icsr-ai-engine
spec:
  selector:
    app: icsr-ai-engine
  ports:
  - name: http
    port: 8000
    targetPort: 8000
    protocol: TCP
  type: ClusterIP

---
# k8s/production/ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: icsr-ai-ingress
  namespace: icsr-ai-prod
  annotations:
    kubernetes.io/ingress.class: "nginx"
    cert-manager.io/cluster-issuer: "letsencrypt-prod"
    nginx.ingress.kubernetes.io/rate-limit: "100"
    nginx.ingress.kubernetes.io/rate-limit-window: "1m"
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
    nginx.ingress.kubernetes.io/cors-allow-origin: "https://icsr-ai.company.com"
spec:
  tls:
  - hosts:
    - api.icsr-ai.company.com
    secretName: icsr-ai-tls
  rules:
  - host: api.icsr-ai.company.com
    http:
      paths:
      - path: /api
        pathType: Prefix
        backend:
          service:
            name: icsr-ai-backend
            port:
              number: 80
      - path: /ai
        pathType: Prefix
        backend:
          service:
            name: icsr-ai-engine
            port:
              number: 8000

---
# k8s/production/hpa.yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: icsr-ai-backend-hpa
  namespace: icsr-ai-prod
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: icsr-ai-backend
  minReplicas: 3
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
      - type: Percent
        value: 50
        periodSeconds: 60
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 10
        periodSeconds: 60
```

### 7.3 ëª¨ë‹ˆí„°ë§ ë° ë¡œê¹…

#### **<ì½”ë“œ 15>** Prometheus ëª¨ë‹ˆí„°ë§ ì„¤ì •
```yaml
# k8s/monitoring/prometheus-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-config
  namespace: monitoring
data:
  prometheus.yml: |
    global:
      scrape_interval: 15s
      evaluation_interval: 15s
    
    rule_files:
      - "icsr_ai_rules.yml"
    
    alerting:
      alertmanagers:
        - static_configs:
            - targets:
              - alertmanager:9093
    
    scrape_configs:
    - job_name: 'icsr-ai-backend'
      static_configs:
      - targets: ['icsr-ai-backend.icsr-ai-prod:80']
      metrics_path: '/metrics'
      scrape_interval: 30s
      
    - job_name: 'icsr-ai-engine'
      static_configs:
      - targets: ['icsr-ai-engine.icsr-ai-prod:8000']
      metrics_path: '/metrics'
      scrape_interval: 30s
      
    - job_name: 'postgres'
      static_configs:
      - targets: ['postgres-exporter.icsr-ai-prod:9187']
      
    - job_name: 'redis'
      static_configs:
      - targets: ['redis-exporter.icsr-ai-prod:9121']